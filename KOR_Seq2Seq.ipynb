{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"KOR_Seq2Seq.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"14RlYWyvQWEzG7pOHbOLBPM_wCCMv9oUG","authorship_tag":"ABX9TyOuri/D9+RhBqI3cLwGRLay"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"QlNRln8ij7mC","executionInfo":{"status":"ok","timestamp":1649406941862,"user_tz":-540,"elapsed":4000,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}}},"outputs":[],"source":["import os\n","import re\n","\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","import unicodedata\n","import urllib3\n","from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Masking\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","\n","import nltk.translate.bleu_score as bleu\n","from collections import Counter\n","from nltk import ngrams"]},{"cell_type":"markdown","source":["##학습을 위한 병렬 corpus 가져오기\n","source와 target을 각각 병렬적으로 매치한 데이터셋"],"metadata":{"id":"pIh9-gtZryQQ"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"22XaDPjzkKNE","executionInfo":{"status":"ok","timestamp":1649406932993,"user_tz":-540,"elapsed":2151,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}},"outputId":"f6ddd8c2-642c-4eb5-cbd7-e6da9c333b73"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["!unzip -uq \"/content/drive/MyDrive/kor-eng.zip\" -d \"/content\" \n","\n","#영어와 프랑스어를 매치한 데이터 셋\n","#영어가 source 프랑스어가 target"],"metadata":{"id":"ctZi2Qd0kiz0","executionInfo":{"status":"ok","timestamp":1649406941863,"user_tz":-540,"elapsed":14,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["def preprocess_sentence_kor(sent):\n","  # 악센트 제거 함수 호출\n","\n","  # 단어와 구두점 사이에 공백 추가해서 구두점을 구분\n","  # ex) \"I am a student.\" => \"I am a student .\"\n","  sent = re.sub(r\"([?.!,¿])\", r\" \\1\", sent)\n","\n","  # (a-z, A-Z, \".\", \"?\", \"!\", \",\")  영어랑 . ? ! , 제외하고 모두 지움\n","  sent = re.sub(r\"[^a-zA-Zㄱ-ㅎㅏ-ㅣ가-힣 !.?]+\", r\" \", sent)\n","  # 다수 개의 공백을 하나의 공백으로 치환\n","  sent = re.sub(r\"\\s+\", \" \", sent)\n","  return sent\n","\n","\n","def preprocess_sentence_eng(sent):\n","  # 악센트 제거 함수 호출\n","\n","  # 단어와 구두점 사이에 공백 추가해서 구두점을 구분\n","  # ex) \"I am a student.\" => \"I am a student .\"\n","  sent = re.sub(r\"([?.!,¿])\", r\" \\1\", sent)\n","\n","  # (a-z, A-Z, \".\", \"?\", \"!\", \",\")  영어랑 . ? ! , 제외하고 모두 지움\n","  sent = re.sub(r\"[^a-zA-Z!.?]+\", r\" \", sent)\n","  \n","  # 다수 개의 공백을 하나의 공백으로 치환\n","  sent = re.sub(r\"\\s+\", \" \", sent)\n","  return sent"],"metadata":{"id":"ifDy9PUgoUST","executionInfo":{"status":"ok","timestamp":1649406941864,"user_tz":-540,"elapsed":10,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"MpEsdm_vO2mF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install konlpy\n","from konlpy.tag import Okt \n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_ZTCgyh_H2rU","executionInfo":{"status":"ok","timestamp":1649406945261,"user_tz":-540,"elapsed":3407,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}},"outputId":"f6375477-ae74-4202-86ce-f75640750ed9"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: konlpy in /usr/local/lib/python3.7/dist-packages (0.6.0)\n","Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n","Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.21.5)\n","Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.3.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.10.0.2)\n"]}]},{"cell_type":"code","source":["num_samples =3000 \n","okt = Okt()\n","\n","def load_preprocessed_data():\n","  encoder_input, decoder_input, decoder_target = [], [], [] \n","  #input을 3개 만듦 encoder는 output이 없으므로 encoder target은 만들 필요 x\n","  with open(\"kor.txt\", \"r\") as lines:\n","    for i, line in enumerate(lines): #line 하나 안에 tab을 기준으로 source와 target을 구분하고 있음\n","      # source 데이터와 target 데이터를 tab을 기준으로 분리\n","      tar_line, src_line, _ = line.strip().split('\\t')\n","\n","      # source 데이터 전처리\n","      src_line = [w for w in okt.morphs(preprocess_sentence_kor(src_line))]\n","     # target 데이터 전처리\n","      tar_line = preprocess_sentence_eng(tar_line)\n","      tar_line_in = [w for w in (\"<sos> \" + tar_line).split()] #line을 받아와서 sos 토큰을 넣어준다\n","      tar_line_out = [w for w in (tar_line + \" <eos>\").split()]\n","\n","      encoder_input.append(src_line)\n","      decoder_input.append(tar_line_in)\n","      decoder_target.append(tar_line_out)\n","\n","      if i == num_samples - 1:\n","        break\n","  lines.close()\n","  return encoder_input, decoder_input, decoder_target\n","\n","sents_kor_in , sents_en_in, sents_en_out = load_preprocessed_data()"],"metadata":{"id":"q0ltJWJ3mAwE","executionInfo":{"status":"ok","timestamp":1649406966703,"user_tz":-540,"elapsed":21446,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["print('인코더의 입력 :',sents_kor_in[15])\n","print('디코더의 입력 :',sents_en_in[25])\n","print('디코더의 레이블 :',sents_en_out[25])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YV53HrgUuJee","executionInfo":{"status":"ok","timestamp":1649406966703,"user_tz":-540,"elapsed":9,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}},"outputId":"0ecdaf8c-4da2-4b83-dec5-2d34e04f0a9f"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["인코더의 입력 : ['알았어', '.']\n","디코더의 입력 : ['<sos>', 'Get', 'up', '.']\n","디코더의 레이블 : ['Get', 'up', '.', '<eos>']\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"y9SkeHwZJaq1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Seq2Seq에 넣기 위해서 tokenize\n","\n","영어와 한국어 각각에 대해서 tokenize함\n"],"metadata":{"id":"LBBnRubLwq7M"}},{"cell_type":"code","source":["#tokenizer 안에 filters는 문장 내에서 \"\"안에 것을 filtering함 우리는 이미 전처리 다해놔서 거를 것 없음 lower도 이미 해놓음\n","\n","tokenizer_kor = Tokenizer(filters=\"\", lower=False)\n","tokenizer_kor.fit_on_texts(sents_kor_in)\n","\n","tokenizer_en = Tokenizer(filters=\"\", lower=False)\n","tokenizer_en.fit_on_texts(sents_en_in)\n","tokenizer_en.fit_on_texts(sents_en_out)\n","\n","\n","encoder_input = tokenizer_kor.texts_to_sequences(sents_kor_in)\n","encoder_input = pad_sequences(encoder_input, padding=\"post\")\n","#padding에는 2가지 pre, post 존재 post는 0들을 뒤에 채우는 것\n","\n","decoder_input = tokenizer_en.texts_to_sequences(sents_en_in)\n","decoder_input = pad_sequences(decoder_input, padding=\"post\")\n","\n","decoder_target = tokenizer_en.texts_to_sequences(sents_en_out)\n","decoder_target = pad_sequences(decoder_target, padding=\"post\")"],"metadata":{"id":"XP8PCJ5FvuPT","executionInfo":{"status":"ok","timestamp":1649406966704,"user_tz":-540,"elapsed":7,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["encoder_input[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mjUVaS5ixqe5","executionInfo":{"status":"ok","timestamp":1649406967481,"user_tz":-540,"elapsed":783,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}},"outputId":"832c3453-0195-4ef7-8507-f9c96c4b9332"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([8, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["#영어는 문장 최대 길이가 12, 한국어는 16\n","\n","print('인코더의 입력의 크기(shape) :',encoder_input.shape)\n","print('디코더의 입력의 크기(shape) :',decoder_input.shape)\n","print('디코더의 레이블의 크기(shape) :',decoder_target.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Gj0X4fGnx3cP","executionInfo":{"status":"ok","timestamp":1649406967482,"user_tz":-540,"elapsed":25,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}},"outputId":"f17e2331-ed72-43e0-b8bb-d3d4dae631df"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["인코더의 입력의 크기(shape) : (3000, 16)\n","디코더의 입력의 크기(shape) : (3000, 12)\n","디코더의 레이블의 크기(shape) : (3000, 12)\n"]}]},{"cell_type":"code","source":["#word_index는 각 단어에 대한 index를 매칭해서 dictionary로 반환\n","src_vocab_size = len(tokenizer_kor.word_index) + 1\n","tar_vocab_size = len(tokenizer_en.word_index) + 1\n","print(\"한국어 단어 집합의 크기 : {:d}, 영어 단어 집합의 크기 : {:d}\".format(src_vocab_size, tar_vocab_size))"],"metadata":{"id":"3qUWf1Pcx9y2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649406967483,"user_tz":-540,"elapsed":24,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}},"outputId":"986d3cf6-3375-4409-9af0-c1c261076bcf"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["한국어 단어 집합의 크기 : 3157, 영어 단어 집합의 크기 : 2203\n"]}]},{"cell_type":"code","source":["#word_index는 단어 - 인덱스 순의 dictionary\n","#index_word는 그 반대 아래의 딕셔너리는 이후 예측값과 실제값 예측에 사용\n","\n","src_to_index = tokenizer_kor.word_index\n","index_to_src = tokenizer_kor.index_word\n","tar_to_index = tokenizer_en.word_index\n","index_to_tar = tokenizer_en.index_word"],"metadata":{"id":"QQI6x7IOyJ9t","executionInfo":{"status":"ok","timestamp":1649406967483,"user_tz":-540,"elapsed":21,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["\n","indices = np.arange(num_samples)\n","np.random.shuffle(indices)\n","encoder_input = encoder_input[indices]\n","decoder_input = decoder_input[indices]\n","decoder_target = decoder_target[indices]"],"metadata":{"id":"wfbz9UKPyy0Z","executionInfo":{"status":"ok","timestamp":1649406967484,"user_tz":-540,"elapsed":22,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["val_num=int(0.1*num_samples) #10% 만큼 test에 사용 split 해준다\n","encoder_input_train = encoder_input[:-val_num]\n","decoder_input_train = decoder_input[:-val_num]\n","decoder_target_train = decoder_target[:-val_num]\n","\n","encoder_input_test = encoder_input[-val_num:]\n","decoder_input_test = decoder_input[-val_num:]\n","decoder_target_test = decoder_target[-val_num:]"],"metadata":{"id":"kTel2uR90fRg","executionInfo":{"status":"ok","timestamp":1649406967484,"user_tz":-540,"elapsed":21,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["print('훈련 source 데이터의 크기 :',encoder_input_train.shape)\n","print('훈련 target 데이터의 크기 :',decoder_input_train.shape)\n","print('훈련 target 레이블의 크기 :',decoder_target_train.shape)\n","print('테스트 source 데이터의 크기 :',encoder_input_test.shape)\n","print('테스트 target 데이터의 크기 :',decoder_input_test.shape)\n","print('테스트 target 레이블의 크기 :',decoder_target_test.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XGLmZFeb3FgA","executionInfo":{"status":"ok","timestamp":1649406967485,"user_tz":-540,"elapsed":22,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}},"outputId":"13b98039-1d3e-496d-d804-9b7d82288480"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["훈련 source 데이터의 크기 : (2700, 16)\n","훈련 target 데이터의 크기 : (2700, 12)\n","훈련 target 레이블의 크기 : (2700, 12)\n","테스트 source 데이터의 크기 : (300, 16)\n","테스트 target 데이터의 크기 : (300, 12)\n","테스트 target 레이블의 크기 : (300, 12)\n"]}]},{"cell_type":"markdown","source":["##본격적인 lstm Seq2Seq 모델링\n","\n","functional API로 쌓음  아래에 functional API에 대한 설명 있음\n","\n","https://www.tensorflow.org/guide/keras/functional?hl=ko"],"metadata":{"id":"KyEZdCrX6rxE"}},{"cell_type":"code","source":["embedding_dim = 64\n","hidden_units = 64"],"metadata":{"id":"eHHotIaP3vSi","executionInfo":{"status":"ok","timestamp":1649406967485,"user_tz":-540,"elapsed":19,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["# 인코더\n","encoder_inputs = Input(shape=(None,))\n","enc_emb = Embedding(src_vocab_size, embedding_dim)(encoder_inputs) # 임베딩 층\n","enc_masking = Masking(mask_value=0.0)(enc_emb) # 패딩 0은 연산에서 제외 #masgking이라는게 있는데 패딩시 발생한 0을 연산에서 아예 무시\n","encoder_lstm = LSTM(hidden_units, return_state=True) # encoder의 상태를 decoder로 보내려면 return state가 true여야한다.\n","encoder_outputs, state_h, state_c = encoder_lstm(enc_masking) # 은닉 상태와 셀 상태를 리턴\n","encoder_states = [state_h, state_c] # 인코더의 은닉 상태와 셀 상태를 저장"],"metadata":{"id":"8BulRhv86wyQ","executionInfo":{"status":"ok","timestamp":1649406968495,"user_tz":-540,"elapsed":1029,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["# 디코더\n","decoder_inputs = Input(shape=(None,))\n","dec_emb_layer = Embedding(tar_vocab_size, hidden_units) # 임베딩 층\n","dec_emb = dec_emb_layer(decoder_inputs) # 패딩 0은 연산에서 제외\n","dec_masking = Masking(mask_value=0.0)(dec_emb)\n","\n","# 상태값 리턴을 위해 return_state는 True, 모든 시점에 대해서 단어를 예측하기 위해 return_sequences는 True\n","decoder_lstm = LSTM(hidden_units, return_sequences=True, return_state=True)  \n","\n","# 인코더의 은닉 상태를 초기 은닉 상태(initial_state)로 사용\n","decoder_outputs, _, _ = decoder_lstm(dec_masking,\n","                                     initial_state=encoder_states)\n","\n","# 모든 시점의 결과에 대해서 소프트맥스 함수를 사용한 출력층을 통해 단어 예측\n","decoder_dense = Dense(tar_vocab_size, activation='softmax') #vocab size만큼의 단어 분포 나오고 거기서 softmax\n","decoder_outputs = decoder_dense(decoder_outputs) #output을 dense에 넣어줌\n","\n","# 모델의 입력과 출력을 정의.\n","model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n","\n","model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])\n","#sparse cross entropy는 결과 값을 one-hot coding하지 않았을 때 cross entropy를 구해주는 loss function"],"metadata":{"id":"yOXL9XES9QUJ","executionInfo":{"status":"ok","timestamp":1649406970186,"user_tz":-540,"elapsed":1693,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["decoder_outputs.shape"],"metadata":{"id":"PYtjwpNTkV8m","executionInfo":{"status":"ok","timestamp":1649406970187,"user_tz":-540,"elapsed":11,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}},"outputId":"bf72ae18-a136-41e9-eb47-283a24d0c435","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TensorShape([None, None, 2203])"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tl769iR5boCw","executionInfo":{"status":"ok","timestamp":1649406970563,"user_tz":-540,"elapsed":384,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}},"outputId":"9bad6ca1-90e2-4751-cac2-aa33ae6abeb2"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_1 (InputLayer)           [(None, None)]       0           []                               \n","                                                                                                  \n"," input_2 (InputLayer)           [(None, None)]       0           []                               \n","                                                                                                  \n"," embedding (Embedding)          (None, None, 64)     202048      ['input_1[0][0]']                \n","                                                                                                  \n"," embedding_1 (Embedding)        (None, None, 64)     140992      ['input_2[0][0]']                \n","                                                                                                  \n"," masking (Masking)              (None, None, 64)     0           ['embedding[0][0]']              \n","                                                                                                  \n"," masking_1 (Masking)            (None, None, 64)     0           ['embedding_1[0][0]']            \n","                                                                                                  \n"," lstm (LSTM)                    [(None, 64),         33024       ['masking[0][0]']                \n","                                 (None, 64),                                                      \n","                                 (None, 64)]                                                      \n","                                                                                                  \n"," lstm_1 (LSTM)                  [(None, None, 64),   33024       ['masking_1[0][0]',              \n","                                 (None, 64),                      'lstm[0][1]',                   \n","                                 (None, 64)]                      'lstm[0][2]']                   \n","                                                                                                  \n"," dense (Dense)                  (None, None, 2203)   143195      ['lstm_1[0][0]']                 \n","                                                                                                  \n","==================================================================================================\n","Total params: 552,283\n","Trainable params: 552,283\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"]}]},{"cell_type":"code","source":["model.fit(x=[encoder_input_train, decoder_input_train], y=decoder_target_train, \\\n","          validation_data=([encoder_input_test, decoder_input_test], decoder_target_test),\n","          batch_size=64, epochs=25)"],"metadata":{"id":"vLmDMNJiAfw7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649407129792,"user_tz":-540,"elapsed":159235,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}},"outputId":"697873d9-4264-47a3-9120-26cc5a817ede"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/25\n","43/43 [==============================] - 28s 224ms/step - loss: 6.5307 - acc: 0.4058 - val_loss: 4.4558 - val_acc: 0.4064\n","Epoch 2/25\n","43/43 [==============================] - 4s 96ms/step - loss: 3.6159 - acc: 0.4307 - val_loss: 3.4731 - val_acc: 0.4064\n","Epoch 3/25\n","43/43 [==============================] - 4s 88ms/step - loss: 3.2326 - acc: 0.4307 - val_loss: 3.3093 - val_acc: 0.4064\n","Epoch 4/25\n","43/43 [==============================] - 4s 86ms/step - loss: 3.0555 - acc: 0.4332 - val_loss: 3.1480 - val_acc: 0.4447\n","Epoch 5/25\n","43/43 [==============================] - 4s 85ms/step - loss: 2.9047 - acc: 0.4855 - val_loss: 3.0346 - val_acc: 0.4778\n","Epoch 6/25\n","43/43 [==============================] - 4s 86ms/step - loss: 2.8020 - acc: 0.5082 - val_loss: 2.9518 - val_acc: 0.4986\n","Epoch 7/25\n","43/43 [==============================] - 4s 87ms/step - loss: 2.7130 - acc: 0.5572 - val_loss: 2.8784 - val_acc: 0.5733\n","Epoch 8/25\n","43/43 [==============================] - 4s 86ms/step - loss: 2.6332 - acc: 0.6026 - val_loss: 2.8102 - val_acc: 0.5825\n","Epoch 9/25\n","43/43 [==============================] - 4s 87ms/step - loss: 2.5583 - acc: 0.6055 - val_loss: 2.7483 - val_acc: 0.5839\n","Epoch 10/25\n","43/43 [==============================] - 4s 85ms/step - loss: 2.4900 - acc: 0.6070 - val_loss: 2.6952 - val_acc: 0.5906\n","Epoch 11/25\n","43/43 [==============================] - 4s 86ms/step - loss: 2.4267 - acc: 0.6144 - val_loss: 2.6450 - val_acc: 0.5942\n","Epoch 12/25\n","43/43 [==============================] - 4s 96ms/step - loss: 2.3674 - acc: 0.6188 - val_loss: 2.5991 - val_acc: 0.5983\n","Epoch 13/25\n","43/43 [==============================] - 4s 85ms/step - loss: 2.3102 - acc: 0.6230 - val_loss: 2.5640 - val_acc: 0.6028\n","Epoch 14/25\n","43/43 [==============================] - 4s 85ms/step - loss: 2.2600 - acc: 0.6269 - val_loss: 2.5269 - val_acc: 0.6039\n","Epoch 15/25\n","43/43 [==============================] - 4s 86ms/step - loss: 2.2151 - acc: 0.6293 - val_loss: 2.5026 - val_acc: 0.6050\n","Epoch 16/25\n","43/43 [==============================] - 4s 86ms/step - loss: 2.1740 - acc: 0.6329 - val_loss: 2.4819 - val_acc: 0.6089\n","Epoch 17/25\n","43/43 [==============================] - 4s 86ms/step - loss: 2.1378 - acc: 0.6387 - val_loss: 2.4648 - val_acc: 0.6119\n","Epoch 18/25\n","43/43 [==============================] - 4s 85ms/step - loss: 2.1029 - acc: 0.6452 - val_loss: 2.4424 - val_acc: 0.6181\n","Epoch 19/25\n","43/43 [==============================] - 4s 86ms/step - loss: 2.0699 - acc: 0.6497 - val_loss: 2.4324 - val_acc: 0.6183\n","Epoch 20/25\n","43/43 [==============================] - 4s 86ms/step - loss: 2.0375 - acc: 0.6561 - val_loss: 2.4151 - val_acc: 0.6233\n","Epoch 21/25\n","43/43 [==============================] - 4s 86ms/step - loss: 2.0061 - acc: 0.6613 - val_loss: 2.3961 - val_acc: 0.6239\n","Epoch 22/25\n","43/43 [==============================] - 4s 86ms/step - loss: 1.9737 - acc: 0.6651 - val_loss: 2.3782 - val_acc: 0.6289\n","Epoch 23/25\n","43/43 [==============================] - 4s 86ms/step - loss: 1.9416 - acc: 0.6681 - val_loss: 2.3624 - val_acc: 0.6319\n","Epoch 24/25\n","43/43 [==============================] - 4s 88ms/step - loss: 1.9160 - acc: 0.6710 - val_loss: 2.3594 - val_acc: 0.6347\n","Epoch 25/25\n","43/43 [==============================] - 4s 88ms/step - loss: 1.8845 - acc: 0.6747 - val_loss: 2.3375 - val_acc: 0.6372\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f1ebb9f6d50>"]},"metadata":{},"execution_count":21}]},{"cell_type":"markdown","source":["## 학습 이후 학습 된 모델로 machine translation 실행"],"metadata":{"id":"-abSS9tRBg5S"}},{"cell_type":"code","source":["# 인코더는 train에서 사용되고 이미 train된 애들을 그대로 가져옵니다\n","encoder_model = Model(encoder_inputs, encoder_states)\n","\n","# 디코더 설계 시작\n","# 이전 시점의 상태를 보관할 텐서\n","decoder_state_input_h = Input(shape=(hidden_units,))\n","decoder_state_input_c = Input(shape=(hidden_units,))\n","decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n","\n","# 훈련 때 사용했던 임베딩 층을 재사용\n","dec_emb2 = dec_emb_layer(decoder_inputs)\n","\n","# 다음 단어 예측을 위해 이전 시점의 상태를 현 시점의 초기 상태로 사용\n","decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\n","#initial state에 decoder state input h decoder state input c를 넣어줌 \n","decoder_states2 = [state_h2, state_c2]\n","\n","# 모든 시점에 대해서 단어 예측\n","decoder_outputs2 = decoder_dense(decoder_outputs2) #output을 desne에 넣어서 확률 처리\n","\n","# 수정된 디코더\n","decoder_model = Model(\n","    [decoder_inputs] + decoder_states_inputs,\n","    [decoder_outputs2] + decoder_states2)"],"metadata":{"id":"qOBVmsreBlQ3","executionInfo":{"status":"ok","timestamp":1649407130173,"user_tz":-540,"elapsed":396,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["#디코더를 컨트롤하기 위한 함수, test과정에서 사용할 함수\n","\n","def decode_sequence(input_seq):\n","  # 입력으로부터 인코더의 마지막 시점의 상태(은닉 상태, 셀 상태)를 얻음\n","  states_value = encoder_model.predict(input_seq)\n","\n","  # <SOS>에 해당하는 정수 생성\n","  target_seq = np.zeros((1,1))\n","  target_seq[0, 0] = tar_to_index['<sos>'] #시작토큰의 inex를 시작에 넣어줌\n","\n","  stop_condition = False\n","  decoded_sentence = ''\n","\n","  # stop_condition이 True가 될 때까지 루프 반복\n","  # 구현의 간소화를 위해서 이 함수는 배치 크기를 1로 가정합니다.\n","  while not stop_condition:\n","    # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n","    output_tokens, h, c = decoder_model.predict([target_seq] + states_value) \n","\n","    # 예측 결과를 단어로 변환\n","    sampled_token_index = np.argmax(output_tokens[0, -1, :])\n","    sampled_char = index_to_tar[sampled_token_index]\n","\n","    # 현재 시점의 예측 단어를 예측 문장에 추가\n","    decoded_sentence += ' '+sampled_char\n","\n","    # <eos>에 도달하거나 정해진 길이를 넘으면 중단.\n","    if (sampled_char == '<eos>' or\n","        len(decoded_sentence) > 50):\n","        stop_condition = True\n","\n","    # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n","    target_seq = np.zeros((1,1))\n","    target_seq[0, 0] = sampled_token_index\n","\n","    # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n","    states_value = [h, c]\n","\n","  return decoded_sentence"],"metadata":{"id":"ysktHo6wXCMX","executionInfo":{"status":"ok","timestamp":1649407130174,"user_tz":-540,"elapsed":5,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["# 원문의 정수 시퀀스를 텍스트 시퀀스로 변환\n","def seq_to_src(input_seq):\n","  sentence = ''\n","  for encoded_word in input_seq:\n","    if(encoded_word != 0):\n","      sentence = sentence + index_to_src[encoded_word] + ' '\n","  return sentence\n","\n","# 번역문의 정수 시퀀스를 텍스트 시퀀스로 변환\n","def seq_to_tar(input_seq):\n","  sentence = ''\n","  for encoded_word in input_seq:\n","    if(encoded_word != 0 and encoded_word != tar_to_index['<sos>'] and encoded_word != tar_to_index['<eos>']):\n","      sentence = sentence + index_to_tar[encoded_word] + ' '\n","  return sentence"],"metadata":{"id":"b2CA_lYdZssM","executionInfo":{"status":"ok","timestamp":1649407130175,"user_tz":-540,"elapsed":6,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["for seq_index in [3, 50, 100, 300, 1001]:\n","  input_seq = encoder_input_train[seq_index: seq_index + 1]\n","  decoded_sentence = decode_sequence(input_seq)\n","\n","  print(\"입력문장 :\",seq_to_src(encoder_input_train[seq_index]))\n","  print(\"정답문장 :\",seq_to_tar(decoder_input_train[seq_index]))\n","  print(\"번역문장 :\",decoded_sentence[1:-5])\n","  print(\"-\"*50)"],"metadata":{"id":"Gc_5xnbXaXmT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649407725187,"user_tz":-540,"elapsed":2150,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}},"outputId":"34be3bc4-ce1f-4f1e-ee94-ad32ec73ed23"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["입력문장 : 그 아빠 에 그 아들 . \n","정답문장 : Like father like son . \n","번역문장 : The re a a age . \n","--------------------------------------------------\n","입력문장 : 우리 가 살아남았어 ! \n","정답문장 : We survived ! \n","번역문장 : They ! \n","--------------------------------------------------\n","입력문장 : 지금 바빠 ? \n","정답문장 : Are you busy now ? \n","번역문장 : You can you ? \n","--------------------------------------------------\n","입력문장 : 나 는 그것 을 버리고 싶지 않다 . \n","정답문장 : I don t want to throw that away . \n","번역문장 : I m a a a a ? \n","--------------------------------------------------\n","입력문장 : 그건 네 책임 이야 . \n","정답문장 : That s your funeral . \n","번역문장 : You is a . \n","--------------------------------------------------\n"]}]}]}