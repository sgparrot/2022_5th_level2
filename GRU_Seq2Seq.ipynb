{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"GRU_Seq2Seq.ipynb","provenance":[{"file_id":"14RlYWyvQWEzG7pOHbOLBPM_wCCMv9oUG","timestamp":1648904550856}],"collapsed_sections":[],"mount_file_id":"14RlYWyvQWEzG7pOHbOLBPM_wCCMv9oUG","authorship_tag":"ABX9TyP3VbFDKuO3eMNVCELNqjIl"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"QlNRln8ij7mC","executionInfo":{"status":"ok","timestamp":1648965175938,"user_tz":-540,"elapsed":4546,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}}},"outputs":[],"source":["import os\n","import re\n","\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","import unicodedata\n","import urllib3\n","from tensorflow.keras.layers import Input, GRU, Embedding, Dense, Masking\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","\n","import nltk.translate.bleu_score as bleu\n","from collections import Counter\n","from nltk import ngrams"]},{"cell_type":"markdown","source":["##학습을 위한 병렬 corpus 가져오기\n","source와 target을 각각 병렬적으로 매치한 데이터셋"],"metadata":{"id":"pIh9-gtZryQQ"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"22XaDPjzkKNE","executionInfo":{"status":"ok","timestamp":1648965169416,"user_tz":-540,"elapsed":21031,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}},"outputId":"e9a862c4-94c4-4ce4-bfe2-4a7aabd5bd0d"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!unzip -uq \"/content/drive/MyDrive/fra-eng.zip\" -d \"/content\" \n","\n","#영어와 프랑스어를 매치한 데이터 셋\n","#영어가 source 프랑스어가 target"],"metadata":{"id":"ctZi2Qd0kiz0","executionInfo":{"status":"ok","timestamp":1648965176962,"user_tz":-540,"elapsed":1031,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["def to_ascii(s):\n","  # 프랑스어 악센트(accent) 삭제\n","  # 예시 : 'déjà diné' -> deja dine\n","  return ''.join(c for c in unicodedata.normalize('NFD', s)\n","                   if unicodedata.category(c) != 'Mn')\n","\n","def preprocess_sentence(sent):\n","  # 악센트 제거 함수 호출\n","  sent = to_ascii(sent.lower())\n","\n","  # 단어와 구두점 사이에 공백 추가해서 구두점을 구분\n","  # ex) \"I am a student.\" => \"I am a student .\"\n","  sent = re.sub(r\"([?.!,¿])\", r\" \\1\", sent)\n","\n","  # (a-z, A-Z, \".\", \"?\", \"!\", \",\")  영어랑 . ? ! , 제외하고 모두 지움\n","  sent = re.sub(r\"[^a-zA-Z!.?]+\", r\" \", sent)\n","\n","  # 다수 개의 공백을 하나의 공백으로 치환\n","  sent = re.sub(r\"\\s+\", \" \", sent)\n","  return sent"],"metadata":{"id":"ifDy9PUgoUST","executionInfo":{"status":"ok","timestamp":1648965176962,"user_tz":-540,"elapsed":5,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["num_samples =40000 #원래는 19만개의 문장이 있는데 3만개정도만 사용\n","\n","def load_preprocessed_data():\n","  encoder_input, decoder_input, decoder_target = [], [], [] \n","  #input을 3개 만듦 encoder는 output이 없으므로 encoder target은 만들 필요 x\n","  with open(\"fra.txt\", \"r\") as lines:\n","    for i, line in enumerate(lines): #line 하나 안에 tab을 기준으로 source와 target을 구분하고 있음\n","      # source 데이터와 target 데이터를 tab을 기준으로 분리\n","      src_line, tar_line, _ = line.strip().split('\\t')\n","\n","      # source 데이터 전처리\n","      src_line = [w for w in preprocess_sentence(src_line).split()]\n","\n","      # target 데이터 전처리\n","      tar_line = preprocess_sentence(tar_line)\n","      tar_line_in = [w for w in (\"<sos> \" + tar_line).split()] #line을 받아와서 sos 토큰을 넣어준다\n","      tar_line_out = [w for w in (tar_line + \" <eos>\").split()] #line을 받아와서 eos 토큰을 넣어준다.\n","\n","      encoder_input.append(src_line)\n","      decoder_input.append(tar_line_in)\n","      decoder_target.append(tar_line_out)\n","\n","      if i == num_samples - 1:\n","        break\n","  lines.close()\n","  return encoder_input, decoder_input, decoder_target\n","\n","sents_en_in , sents_fra_in, sents_fra_out = load_preprocessed_data()"],"metadata":{"id":"q0ltJWJ3mAwE","executionInfo":{"status":"ok","timestamp":1648965179348,"user_tz":-540,"elapsed":2389,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["print('인코더의 입력 :',sents_en_in[15])\n","print('디코더의 입력 :',sents_fra_in[15])\n","print('디코더의 레이블 :',sents_fra_out[15])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YV53HrgUuJee","executionInfo":{"status":"ok","timestamp":1648965179348,"user_tz":-540,"elapsed":6,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}},"outputId":"d9a94025-7e6b-4dab-a28c-299fd8ff41fc"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["인코더의 입력 : ['run', '.']\n","디코더의 입력 : ['<sos>', 'prenez', 'vos', 'jambes', 'a', 'vos', 'cous', '!']\n","디코더의 레이블 : ['prenez', 'vos', 'jambes', 'a', 'vos', 'cous', '!', '<eos>']\n"]}]},{"cell_type":"markdown","source":["##Seq2Seq에 넣기 위해서 tokenize\n","\n","영어와 프랑스어 각각에 대해서 tokenize함\n"],"metadata":{"id":"LBBnRubLwq7M"}},{"cell_type":"code","source":["#tokenizer 안에 filters는 문장 내에서 \"\"안에 것을 filtering함 우리는 이미 전처리 다해놔서 거를 것 없음 lower도 이미 해놓음\n","\n","tokenizer_en = Tokenizer(filters=\"\", lower=False)\n","tokenizer_en.fit_on_texts(sents_en_in)\n","\n","tokenizer_fra = Tokenizer(filters=\"\", lower=False)\n","tokenizer_fra.fit_on_texts(sents_fra_in)\n","tokenizer_fra.fit_on_texts(sents_fra_out)\n","\n","\n","encoder_input = tokenizer_en.texts_to_sequences(sents_en_in)\n","encoder_input = pad_sequences(encoder_input, padding=\"post\")\n","#padding에는 2가지 pre, post 존재 post는 0들을 뒤에 채우는 것\n","\n","decoder_input = tokenizer_fra.texts_to_sequences(sents_fra_in)\n","decoder_input = pad_sequences(decoder_input, padding=\"post\")\n","\n","decoder_target = tokenizer_fra.texts_to_sequences(sents_fra_out)\n","decoder_target = pad_sequences(decoder_target, padding=\"post\")"],"metadata":{"id":"XP8PCJ5FvuPT","executionInfo":{"status":"ok","timestamp":1648965180230,"user_tz":-540,"elapsed":885,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["encoder_input[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mjUVaS5ixqe5","executionInfo":{"status":"ok","timestamp":1648965180231,"user_tz":-540,"elapsed":14,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}},"outputId":"378a68d9-dce1-4bd5-84fc-5c5c6c1579bf"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([31,  1,  0,  0,  0,  0,  0,  0], dtype=int32)"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["#영어는 문장 최대 길이가 8인 반면 프랑스어는 최대 길이가 16\n","\n","print('인코더의 입력의 크기(shape) :',encoder_input.shape)\n","print('디코더의 입력의 크기(shape) :',decoder_input.shape)\n","print('디코더의 레이블의 크기(shape) :',decoder_target.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Gj0X4fGnx3cP","executionInfo":{"status":"ok","timestamp":1648965180232,"user_tz":-540,"elapsed":7,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}},"outputId":"b71e42dd-76ab-41cb-ea75-f05b4051ec02"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["인코더의 입력의 크기(shape) : (40000, 8)\n","디코더의 입력의 크기(shape) : (40000, 16)\n","디코더의 레이블의 크기(shape) : (40000, 16)\n"]}]},{"cell_type":"code","source":["#word_index는 각 단어에 대한 index를 매칭해서 dictionary로 반환\n","src_vocab_size = len(tokenizer_en.word_index) + 1\n","tar_vocab_size = len(tokenizer_fra.word_index) + 1\n","print(\"영어 단어 집합의 크기 : {:d}, 프랑스어 단어 집합의 크기 : {:d}\".format(src_vocab_size, tar_vocab_size))"],"metadata":{"id":"3qUWf1Pcx9y2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648965180625,"user_tz":-540,"elapsed":398,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}},"outputId":"1e8835ed-b6a7-4c91-fa2a-ad71385d8ed2"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["영어 단어 집합의 크기 : 5240, 프랑스어 단어 집합의 크기 : 9048\n"]}]},{"cell_type":"code","source":["#word_index는 단어 - 인덱스 순의 dictionary\n","#index_word는 그 반대 아래의 딕셔너리는 이후 예측값과 실제값 예측에 사용\n","\n","src_to_index = tokenizer_en.word_index\n","index_to_src = tokenizer_en.index_word\n","tar_to_index = tokenizer_fra.word_index\n","index_to_tar = tokenizer_fra.index_word"],"metadata":{"id":"QQI6x7IOyJ9t","executionInfo":{"status":"ok","timestamp":1648965180625,"user_tz":-540,"elapsed":6,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":[""],"metadata":{"id":"eCnZipqkYgSU"}},{"cell_type":"code","source":["#33000개의 문장을 무작위로 shuffle\n","indices = np.arange(num_samples)\n","np.random.shuffle(indices)\n","encoder_input = encoder_input[indices]\n","decoder_input = decoder_input[indices]\n","decoder_target = decoder_target[indices]"],"metadata":{"id":"wfbz9UKPyy0Z","executionInfo":{"status":"ok","timestamp":1648965180626,"user_tz":-540,"elapsed":7,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["val_num=int(0.1*num_samples) #10% 만큼 test에 사용 split 해준다\n","encoder_input_train = encoder_input[:-val_num]\n","decoder_input_train = decoder_input[:-val_num]\n","decoder_target_train = decoder_target[:-val_num]\n","\n","encoder_input_test = encoder_input[-val_num:]\n","decoder_input_test = decoder_input[-val_num:]\n","decoder_target_test = decoder_target[-val_num:]"],"metadata":{"id":"kTel2uR90fRg","executionInfo":{"status":"ok","timestamp":1648965180626,"user_tz":-540,"elapsed":6,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["print('훈련 source 데이터의 크기 :',encoder_input_train.shape)\n","print('훈련 target 데이터의 크기 :',decoder_input_train.shape)\n","print('훈련 target 레이블의 크기 :',decoder_target_train.shape)\n","print('테스트 source 데이터의 크기 :',encoder_input_test.shape)\n","print('테스트 target 데이터의 크기 :',decoder_input_test.shape)\n","print('테스트 target 레이블의 크기 :',decoder_target_test.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XGLmZFeb3FgA","executionInfo":{"status":"ok","timestamp":1648965180626,"user_tz":-540,"elapsed":6,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}},"outputId":"14b9a590-f268-4da0-cb04-e0fb682cf2e4"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["훈련 source 데이터의 크기 : (36000, 8)\n","훈련 target 데이터의 크기 : (36000, 16)\n","훈련 target 레이블의 크기 : (36000, 16)\n","테스트 source 데이터의 크기 : (4000, 8)\n","테스트 target 데이터의 크기 : (4000, 16)\n","테스트 target 레이블의 크기 : (4000, 16)\n"]}]},{"cell_type":"markdown","source":["##본격적인 lstm Seq2Seq 모델링\n","\n","functional API로 쌓음  아래에 functional API에 대한 설명 있음\n","\n","https://www.tensorflow.org/guide/keras/functional?hl=ko"],"metadata":{"id":"KyEZdCrX6rxE"}},{"cell_type":"code","source":["embedding_dim = 64\n","hidden_units = 64"],"metadata":{"id":"eHHotIaP3vSi","executionInfo":{"status":"ok","timestamp":1648965180627,"user_tz":-540,"elapsed":5,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["# 인코더\n","encoder_inputs = Input(shape=(None,))\n","enc_emb = Embedding(src_vocab_size, embedding_dim)(encoder_inputs) # 임베딩 층\n","enc_masking = Masking(mask_value=0.0)(enc_emb) # 패딩 0은 연산에서 제외 #masgking이라는게 있는데 패딩시 발생한 0을 연산에서 아예 무시\n","encoder_GRU = GRU(hidden_units, return_state=True) # encoder의 상태를 decoder로 보내려면 return state가 true여야한다.\n","encoder_outputs, enc_states = encoder_GRU(enc_masking) # 은닉 상태와 셀 상태를 리턴 \n","encoder_states = [enc_states]\n","#encoder_states = [state_h, state_c] # 인코더의 은닉 상태와 셀 상태를 저장"],"metadata":{"id":"8BulRhv86wyQ","executionInfo":{"status":"ok","timestamp":1648965184658,"user_tz":-540,"elapsed":4036,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["# 디코더\n","decoder_inputs = Input(shape=(None,))\n","dec_emb_layer = Embedding(tar_vocab_size, hidden_units) # 임베딩 층\n","dec_emb = dec_emb_layer(decoder_inputs) # 패딩 0은 연산에서 제외\n","dec_masking = Masking(mask_value=0.0)(dec_emb)\n","\n","# 상태값 리턴을 위해 return_state는 True, 모든 시점에 대해서 단어를 예측하기 위해 return_sequences는 True\n","decoder_GRU = GRU(hidden_units, return_sequences=True, return_state=True)  \n","\n","# 인코더의 은닉 상태를 초기 은닉 상태(initial_state)로 사용\n","decoder_outputs, _ = decoder_GRU(dec_masking,\n","                                     initial_state=encoder_states)\n","\n","# 모든 시점의 결과에 대해서 소프트맥스 함수를 사용한 출력층을 통해 단어 예측\n","decoder_dense = Dense(tar_vocab_size, activation='softmax') #vocab size만큼의 단어 분포 나오고 거기서 softmax\n","decoder_outputs = decoder_dense(decoder_outputs) #output을 dense에 넣어줌\n","\n","# 모델의 입력과 출력을 정의.\n","model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n","\n","model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])\n","#sparse cross entropy는 결과 값을 one-hot coding하지 않았을 때 cross entropy를 구해주는 loss function"],"metadata":{"id":"yOXL9XES9QUJ","executionInfo":{"status":"ok","timestamp":1648965186027,"user_tz":-540,"elapsed":1382,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tl769iR5boCw","executionInfo":{"status":"ok","timestamp":1648965186028,"user_tz":-540,"elapsed":16,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}},"outputId":"8c2b19b9-a8e5-467d-c3b6-2a9c369de036"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_1 (InputLayer)           [(None, None)]       0           []                               \n","                                                                                                  \n"," input_2 (InputLayer)           [(None, None)]       0           []                               \n","                                                                                                  \n"," embedding (Embedding)          (None, None, 64)     335360      ['input_1[0][0]']                \n","                                                                                                  \n"," embedding_1 (Embedding)        (None, None, 64)     579072      ['input_2[0][0]']                \n","                                                                                                  \n"," masking (Masking)              (None, None, 64)     0           ['embedding[0][0]']              \n","                                                                                                  \n"," masking_1 (Masking)            (None, None, 64)     0           ['embedding_1[0][0]']            \n","                                                                                                  \n"," gru (GRU)                      [(None, 64),         24960       ['masking[0][0]']                \n","                                 (None, 64)]                                                      \n","                                                                                                  \n"," gru_1 (GRU)                    [(None, None, 64),   24960       ['masking_1[0][0]',              \n","                                 (None, 64)]                      'gru[0][1]']                    \n","                                                                                                  \n"," dense (Dense)                  (None, None, 9048)   588120      ['gru_1[0][0]']                  \n","                                                                                                  \n","==================================================================================================\n","Total params: 1,552,472\n","Trainable params: 1,552,472\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"]}]},{"cell_type":"code","source":["model.fit(x=[encoder_input_train, decoder_input_train], y=decoder_target_train, \\\n","          validation_data=([encoder_input_test, decoder_input_test], decoder_target_test),\n","          batch_size=128, epochs=5)"],"metadata":{"id":"vLmDMNJiAfw7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648965276155,"user_tz":-540,"elapsed":90132,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}},"outputId":"71cd422a-5948-4fe8-bd34-42950592d9b8"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","282/282 [==============================] - 27s 51ms/step - loss: 3.2492 - acc: 0.6051 - val_loss: 2.0309 - val_acc: 0.6402\n","Epoch 2/5\n","282/282 [==============================] - 11s 39ms/step - loss: 1.8031 - acc: 0.7136 - val_loss: 1.6451 - val_acc: 0.7419\n","Epoch 3/5\n","282/282 [==============================] - 11s 38ms/step - loss: 1.5826 - acc: 0.7444 - val_loss: 1.5434 - val_acc: 0.7500\n","Epoch 4/5\n","282/282 [==============================] - 11s 38ms/step - loss: 1.4925 - acc: 0.7542 - val_loss: 1.4766 - val_acc: 0.7575\n","Epoch 5/5\n","282/282 [==============================] - 11s 39ms/step - loss: 1.4143 - acc: 0.7611 - val_loss: 1.3911 - val_acc: 0.7656\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f09fdda6950>"]},"metadata":{},"execution_count":19}]},{"cell_type":"markdown","source":["## 학습 이후 학습 된 모델로 machine translation 실행"],"metadata":{"id":"-abSS9tRBg5S"}},{"cell_type":"code","source":["# 인코더는 train에서 사용되고 이미 train된 애들을 그대로 가져옵니다\n","encoder_model = Model(encoder_inputs, encoder_states)\n","\n","# 디코더 설계 시작\n","# 이전 시점의 상태를 보관할 텐서\n","decoder_states_input = Input(shape=(hidden_units,))\n","decoder_states_inputs =[decoder_states_input]\n","\n","# 훈련 때 사용했던 임베딩 층을 재사용\n","dec_emb2 = dec_emb_layer(decoder_inputs)\n","\n","# 다음 단어 예측을 위해 이전 시점의 상태를 현 시점의 초기 상태로 사용\n","decoder_outputs2, state = decoder_GRU(dec_emb2, initial_state=decoder_states_inputs)\n","decoder_states2 = [state]\n","# 모든 시점에 대해서 단어 예측\n","decoder_outputs2 = decoder_dense(decoder_outputs2) #output을 desne에 넣어서 확률 처리\n","\n","# 수정된 디코더\n","decoder_model = Model(\n","    [decoder_inputs] + decoder_states_inputs,\n","    [decoder_outputs2] + decoder_states2)"],"metadata":{"id":"qOBVmsreBlQ3","executionInfo":{"status":"ok","timestamp":1648965276155,"user_tz":-540,"elapsed":13,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["#디코더를 컨트롤하기 위한 함수, test과정에서 사용할 함수\n","\n","def decode_sequence(input_seq):\n","  # 입력으로부터 인코더의 마지막 시점의 상태(은닉 상태, 셀 상태)를 얻음\n","  states_value = encoder_model.predict(input_seq)\n","\n","  # <SOS>에 해당하는 정수 생성\n","  target_seq = np.zeros((1,1))\n","  target_seq[0, 0] = tar_to_index['<sos>'] #시작토큰의 inex를 시작에 넣어줌\n","\n","  stop_condition = False\n","  decoded_sentence = ''\n","\n","  # stop_condition이 True가 될 때까지 루프 반복\n","  # 구현의 간소화를 위해서 이 함수는 배치 크기를 1로 가정합니다.\n","  while not stop_condition:\n","    # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n","    output_tokens, states = decoder_model.predict([target_seq] + states_value) \n","\n","    # 예측 결과를 단어로 변환\n","    sampled_token_index = np.argmax(output_tokens[0, -1, :])\n","    sampled_char = index_to_tar[sampled_token_index]\n","\n","    # 현재 시점의 예측 단어를 예측 문장에 추가\n","    decoded_sentence += ' '+sampled_char\n","\n","    # <eos>에 도달하거나 정해진 길이를 넘으면 중단.\n","    if (sampled_char == '<eos>' or\n","        len(decoded_sentence) > 50):\n","        stop_condition = True\n","\n","    # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n","    target_seq = np.zeros((1,1))\n","    target_seq[0, 0] = sampled_token_index\n","\n","    # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n","    states_value = [states]\n","\n","  return decoded_sentence"],"metadata":{"id":"ysktHo6wXCMX","executionInfo":{"status":"ok","timestamp":1648965276156,"user_tz":-540,"elapsed":13,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["# 원문의 정수 시퀀스를 텍스트 시퀀스로 변환\n","def seq_to_src(input_seq):\n","  sentence = ''\n","  for encoded_word in input_seq:\n","    if(encoded_word != 0):\n","      sentence = sentence + index_to_src[encoded_word] + ' '\n","  return sentence\n","\n","# 번역문의 정수 시퀀스를 텍스트 시퀀스로 변환\n","def seq_to_tar(input_seq):\n","  sentence = ''\n","  for encoded_word in input_seq:\n","    if(encoded_word != 0 and encoded_word != tar_to_index['<sos>'] and encoded_word != tar_to_index['<eos>']):\n","      sentence = sentence + index_to_tar[encoded_word] + ' '\n","  return sentence"],"metadata":{"id":"b2CA_lYdZssM","executionInfo":{"status":"ok","timestamp":1648965276156,"user_tz":-540,"elapsed":13,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["for seq_index in [3, 50, 100, 300, 1001]:\n","  input_seq = encoder_input_train[seq_index: seq_index + 1]\n","  decoded_sentence = decode_sequence(input_seq)\n","\n","  print(\"입력문장 :\",seq_to_src(encoder_input_train[seq_index]))\n","  print(\"정답문장 :\",seq_to_tar(decoder_input_train[seq_index]))\n","  print(\"번역문장 :\",decoded_sentence[1:-5])\n","  print('BLEU :',bleu.sentence_bleu(list(map(lambda ref: ref.split(), seq_to_tar(decoder_input_train[seq_index]))),decoded_sentence[1:-5].split()))\n","  print(\"-\"*50)"],"metadata":{"id":"Gc_5xnbXaXmT","colab":{"base_uri":"https://localhost:8080/","height":671},"executionInfo":{"status":"error","timestamp":1648965278523,"user_tz":-540,"elapsed":1898,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}},"outputId":"cda8c58a-2fbe-49a0-c82a-11264a516986"},"execution_count":23,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-7af3d3bb9f3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1001\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0minput_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_input_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseq_index\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mseq_index\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mdecoded_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"입력문장 :\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseq_to_src\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_input_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseq_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-21-57ab8b69404e>\u001b[0m in \u001b[0;36mdecode_sequence\u001b[0;34m(input_seq)\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstop_condition\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0moutput_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_seq\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstates_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# 예측 결과를 단어로 변환\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1148\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1801, in predict_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1790, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1783, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1751, in predict_step\n        return self(x, training=False)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/input_spec.py\", line 200, in assert_input_compatibility\n        raise ValueError(f'Layer \"{layer_name}\" expects {len(input_spec)} input(s),'\n\n    ValueError: Layer \"model_2\" expects 2 input(s), but it received 1 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None, 1, 64) dtype=float32>]\n"]}]},{"cell_type":"code","source":["for seq_index in [3, 50, 100, 300, 1001]:\n","  input_seq = encoder_input_test[seq_index: seq_index + 1]\n","  decoded_sentence = decode_sequence(input_seq)\n","\n","  print(\"입력문장 :\",seq_to_src(encoder_input_test[seq_index]))\n","  print(\"정답문장 :\",seq_to_tar(decoder_input_test[seq_index]))\n","  print(\"번역문장 :\",decoded_sentence[1:-5])\n","  print('BLEU :',bleu.sentence_bleu(list(map(lambda ref: ref.split(), seq_to_tar(decoder_input_test[seq_index]))),decoded_sentence[1:-5].split()))\n","\n","  print(\"-\"*50)"],"metadata":{"id":"DOGEwKWFdMwt","executionInfo":{"status":"aborted","timestamp":1648965278521,"user_tz":-540,"elapsed":7,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"NovNpHFBr_Bx","executionInfo":{"status":"aborted","timestamp":1648965278522,"user_tz":-540,"elapsed":8,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}}},"execution_count":null,"outputs":[]}]}