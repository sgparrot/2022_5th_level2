{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Seq2Seq_2layer.ipynb","provenance":[{"file_id":"14RlYWyvQWEzG7pOHbOLBPM_wCCMv9oUG","timestamp":1648743633709}],"collapsed_sections":[],"mount_file_id":"14RlYWyvQWEzG7pOHbOLBPM_wCCMv9oUG","authorship_tag":"ABX9TyNtaF/PWiS9Ns/yBOuE6k1U"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"QlNRln8ij7mC","executionInfo":{"status":"ok","timestamp":1648800570703,"user_tz":-540,"elapsed":3923,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}}},"outputs":[],"source":["import os\n","import re\n","\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","import unicodedata\n","import urllib3\n","from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Masking\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","\n","import nltk.translate.bleu_score as bleu\n","from collections import Counter\n","from nltk import ngrams"]},{"cell_type":"markdown","source":["##학습을 위한 병렬 corpus 가져오기\n","source와 target을 각각 병렬적으로 매치한 데이터셋"],"metadata":{"id":"pIh9-gtZryQQ"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"22XaDPjzkKNE","executionInfo":{"status":"ok","timestamp":1648800565030,"user_tz":-540,"elapsed":22483,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}},"outputId":"97440280-640a-4e58-a8a2-7f86de02be9a"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!unzip -uq \"/content/drive/MyDrive/fra-eng.zip\" -d \"/content\" \n","\n","#영어와 프랑스어를 매치한 데이터 셋\n","#영어가 source 프랑스어가 target"],"metadata":{"id":"ctZi2Qd0kiz0","executionInfo":{"status":"ok","timestamp":1648800572373,"user_tz":-540,"elapsed":1684,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["def to_ascii(s):\n","  # 프랑스어 악센트(accent) 삭제\n","  # 예시 : 'déjà diné' -> deja dine\n","  return ''.join(c for c in unicodedata.normalize('NFD', s)\n","                   if unicodedata.category(c) != 'Mn')\n","\n","def preprocess_sentence(sent):\n","  # 악센트 제거 함수 호출\n","  sent = to_ascii(sent.lower())\n","\n","  # 단어와 구두점 사이에 공백 추가해서 구두점을 구분\n","  # ex) \"I am a student.\" => \"I am a student .\"\n","  sent = re.sub(r\"([?.!,¿])\", r\" \\1\", sent)\n","\n","  # (a-z, A-Z, \".\", \"?\", \"!\", \",\")  영어랑 . ? ! , 제외하고 모두 지움\n","  sent = re.sub(r\"[^a-zA-Z!.?]+\", r\" \", sent)\n","\n","  # 다수 개의 공백을 하나의 공백으로 치환\n","  sent = re.sub(r\"\\s+\", \" \", sent)\n","  return sent"],"metadata":{"id":"ifDy9PUgoUST","executionInfo":{"status":"ok","timestamp":1648800572375,"user_tz":-540,"elapsed":8,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["num_samples =40000 #원래는 19만개의 문장이 있는데 4만개정도만 사용\n","\n","def load_preprocessed_data():\n","  encoder_input, decoder_input, decoder_target = [], [], [] \n","  #input을 3개 만듦 encoder는 output이 없으므로 encoder target은 만들 필요 x\n","  with open(\"fra.txt\", \"r\") as lines:\n","    for i, line in enumerate(lines): #line 하나 안에 tab을 기준으로 source와 target을 구분하고 있음\n","      # source 데이터와 target 데이터를 tab을 기준으로 분리\n","      src_line, tar_line, _ = line.strip().split('\\t')\n","\n","      # source 데이터 전처리\n","      src_line = [w for w in preprocess_sentence(src_line).split()]\n","\n","      # target 데이터 전처리\n","      tar_line = preprocess_sentence(tar_line)\n","      tar_line_in = [w for w in (\"<sos> \" + tar_line).split()] #line을 받아와서 sos 토큰을 넣어준다\n","      tar_line_out = [w for w in (tar_line + \" <eos>\").split()] #line을 받아와서 eos 토큰을 넣어준다.\n","\n","      encoder_input.append(src_line)\n","      decoder_input.append(tar_line_in)\n","      decoder_target.append(tar_line_out)\n","\n","      if i == num_samples - 1:\n","        break\n","  lines.close()\n","  return encoder_input, decoder_input, decoder_target\n","\n","sents_en_in , sents_fra_in, sents_fra_out = load_preprocessed_data()"],"metadata":{"id":"q0ltJWJ3mAwE","executionInfo":{"status":"ok","timestamp":1648800574319,"user_tz":-540,"elapsed":1951,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["print('인코더의 입력 :',sents_en_in[15])\n","print('디코더의 입력 :',sents_fra_in[15])\n","print('디코더의 레이블 :',sents_fra_out[15])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YV53HrgUuJee","executionInfo":{"status":"ok","timestamp":1648800574320,"user_tz":-540,"elapsed":6,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}},"outputId":"a8aefb4c-5f58-4eb8-a7e8-0830d31e9430"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["인코더의 입력 : ['run', '.']\n","디코더의 입력 : ['<sos>', 'prenez', 'vos', 'jambes', 'a', 'vos', 'cous', '!']\n","디코더의 레이블 : ['prenez', 'vos', 'jambes', 'a', 'vos', 'cous', '!', '<eos>']\n"]}]},{"cell_type":"markdown","source":["##Seq2Seq에 넣기 위해서 tokenize\n","\n","영어와 프랑스어 각각에 대해서 tokenize함\n"],"metadata":{"id":"LBBnRubLwq7M"}},{"cell_type":"code","source":["#tokenizer 안에 filters는 문장 내에서 \"\"안에 것을 filtering함 우리는 이미 전처리 다해놔서 거를 것 없음 lower도 이미 해놓음\n","\n","tokenizer_en = Tokenizer(filters=\"\", lower=False)\n","tokenizer_en.fit_on_texts(sents_en_in)\n","\n","tokenizer_fra = Tokenizer(filters=\"\", lower=False)\n","tokenizer_fra.fit_on_texts(sents_fra_in)\n","tokenizer_fra.fit_on_texts(sents_fra_out)\n","\n","\n","encoder_input = tokenizer_en.texts_to_sequences(sents_en_in)\n","encoder_input = pad_sequences(encoder_input, padding=\"post\")\n","#padding에는 2가지 pre, post 존재 post는 0들을 뒤에 채우는 것\n","\n","decoder_input = tokenizer_fra.texts_to_sequences(sents_fra_in)\n","decoder_input = pad_sequences(decoder_input, padding=\"post\")\n","\n","decoder_target = tokenizer_fra.texts_to_sequences(sents_fra_out)\n","decoder_target = pad_sequences(decoder_target, padding=\"post\")"],"metadata":{"id":"XP8PCJ5FvuPT","executionInfo":{"status":"ok","timestamp":1648800575933,"user_tz":-540,"elapsed":1617,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["encoder_input[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mjUVaS5ixqe5","executionInfo":{"status":"ok","timestamp":1648800575934,"user_tz":-540,"elapsed":13,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}},"outputId":"3fa00960-b2da-4996-9b2d-a19ebeffdbd4"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([31,  1,  0,  0,  0,  0,  0,  0], dtype=int32)"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["#영어는 문장 최대 길이가 8인 반면 프랑스어는 최대 길이가 16\n","\n","print('인코더의 입력의 크기(shape) :',encoder_input.shape)\n","print('디코더의 입력의 크기(shape) :',decoder_input.shape)\n","print('디코더의 레이블의 크기(shape) :',decoder_target.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Gj0X4fGnx3cP","executionInfo":{"status":"ok","timestamp":1648800575934,"user_tz":-540,"elapsed":11,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}},"outputId":"edccde78-7512-43f9-c533-fbca02701863"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["인코더의 입력의 크기(shape) : (40000, 8)\n","디코더의 입력의 크기(shape) : (40000, 16)\n","디코더의 레이블의 크기(shape) : (40000, 16)\n"]}]},{"cell_type":"code","source":["#word_index는 각 단어에 대한 index를 매칭해서 dictionary로 반환\n","src_vocab_size = len(tokenizer_en.word_index) + 1\n","tar_vocab_size = len(tokenizer_fra.word_index) + 1\n","print(\"영어 단어 집합의 크기 : {:d}, 프랑스어 단어 집합의 크기 : {:d}\".format(src_vocab_size, tar_vocab_size))"],"metadata":{"id":"3qUWf1Pcx9y2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648800575935,"user_tz":-540,"elapsed":10,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}},"outputId":"a9c2c2b0-3d25-40f0-acf9-11f5b483d340"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["영어 단어 집합의 크기 : 5240, 프랑스어 단어 집합의 크기 : 9048\n"]}]},{"cell_type":"code","source":["#word_index는 단어 - 인덱스 순의 dictionary\n","#index_word는 그 반대 아래의 딕셔너리는 이후 예측값과 실제값 예측에 사용\n","\n","src_to_index = tokenizer_en.word_index\n","index_to_src = tokenizer_en.index_word\n","tar_to_index = tokenizer_fra.word_index\n","index_to_tar = tokenizer_fra.index_word"],"metadata":{"id":"QQI6x7IOyJ9t","executionInfo":{"status":"ok","timestamp":1648800575935,"user_tz":-540,"elapsed":8,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["#33000개의 문장을 무작위로 shuffle\n","indices = np.arange(num_samples)\n","np.random.shuffle(indices)\n","encoder_input = encoder_input[indices]\n","decoder_input = decoder_input[indices]\n","decoder_target = decoder_target[indices]"],"metadata":{"id":"wfbz9UKPyy0Z","executionInfo":{"status":"ok","timestamp":1648800575935,"user_tz":-540,"elapsed":8,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["val_num=int(0.1*num_samples) #10% 만큼 test에 사용 split 해준다\n","encoder_input_train = encoder_input[:-val_num]\n","decoder_input_train = decoder_input[:-val_num]\n","decoder_target_train = decoder_target[:-val_num]\n","\n","encoder_input_test = encoder_input[-val_num:]\n","decoder_input_test = decoder_input[-val_num:]\n","decoder_target_test = decoder_target[-val_num:]"],"metadata":{"id":"kTel2uR90fRg","executionInfo":{"status":"ok","timestamp":1648800575936,"user_tz":-540,"elapsed":9,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["print('훈련 source 데이터의 크기 :',encoder_input_train.shape)\n","print('훈련 target 데이터의 크기 :',decoder_input_train.shape)\n","print('훈련 target 레이블의 크기 :',decoder_target_train.shape)\n","print('테스트 source 데이터의 크기 :',encoder_input_test.shape)\n","print('테스트 target 데이터의 크기 :',decoder_input_test.shape)\n","print('테스트 target 레이블의 크기 :',decoder_target_test.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XGLmZFeb3FgA","executionInfo":{"status":"ok","timestamp":1648800575936,"user_tz":-540,"elapsed":8,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}},"outputId":"0520ddfd-7e4f-491a-b23d-78b1b74a755f"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["훈련 source 데이터의 크기 : (36000, 8)\n","훈련 target 데이터의 크기 : (36000, 16)\n","훈련 target 레이블의 크기 : (36000, 16)\n","테스트 source 데이터의 크기 : (4000, 8)\n","테스트 target 데이터의 크기 : (4000, 16)\n","테스트 target 레이블의 크기 : (4000, 16)\n"]}]},{"cell_type":"markdown","source":["##본격적인 lstm Seq2Seq 모델링\n","\n","functional API로 쌓음  아래에 functional API에 대한 설명 있음\n","\n","https://www.tensorflow.org/guide/keras/functional?hl=ko"],"metadata":{"id":"KyEZdCrX6rxE"}},{"cell_type":"code","source":["embedding_dim = 64\n","hidden_units = 64"],"metadata":{"id":"eHHotIaP3vSi","executionInfo":{"status":"ok","timestamp":1648800575937,"user_tz":-540,"elapsed":8,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["# 인코더\n","encoder_inputs = Input(shape=(None,))\n","enc_emb = Embedding(src_vocab_size, embedding_dim)(encoder_inputs) \n","enc_masking = Masking(mask_value=0.0)(enc_emb) \n","\n","# 첫 번째 lstm\n","encoder_lstm1 = LSTM(hidden_units, return_sequences=True,return_state=True)  #return_sequences가 추가되었는데 2번째 lstm으로 전달하기 위해서 sequeucne를 열어준다.\n","encoder_outputs1, state_h1, state_c1 = encoder_lstm1(enc_masking) # 첫번째 lstm cell을 돌림 그리고 각각 output, h,c state 저장\n","encoder_states1 = [state_h1, state_c1] # 인코더의 은닉 상태와 셀 상태를 저장\n","\n","\n","#두 번째 lstm\n","encoder_lstm2 = LSTM(hidden_units, return_state=True)\n","encoder_outputs2, state_h2, state_c2 = encoder_lstm2(encoder_outputs1) #output을 lstm2에 넣어준다\n","encoder_states2 = [state_h2,state_c2]\n"],"metadata":{"id":"8BulRhv86wyQ","executionInfo":{"status":"ok","timestamp":1648800581396,"user_tz":-540,"elapsed":5466,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["# 디코더\n","decoder_inputs = Input(shape=(None,))\n","#dec_emb_layer = Embedding(tar_vocab_size, hidden_units) # 임베딩 층\n","dec_emb_layer = Embedding(tar_vocab_size, embedding_dim) # 임베딩 층\n","dec_emb = dec_emb_layer(decoder_inputs)\n","dec_masking = Masking(mask_value=0.0)(dec_emb)\n","\n","# 상태값 리턴을 위해 return_state는 True, 모든 시점에 대해서 단어를 예측하기 위해 return_sequences는 True\n","decoder_lstm1 = LSTM(hidden_units, return_sequences=True, return_state=True)  \n","decoder_lstm2 = LSTM(hidden_units, return_sequences=True, return_state=True)  \n","# 인코더의 은닉 상태를 초기 은닉 상태(initial_state)로 사용\n","decoder_outputs1, _, _ = decoder_lstm1(dec_masking,\n","                                     initial_state=encoder_states1)\n","\n","decoder_outputs2, _, _ = decoder_lstm2(decoder_outputs1,\n","                                     initial_state=encoder_states2)\n","# 모든 시점의 결과에 대해서 소프트맥스 함수를 사용한 출력층을 통해 단어 예측\n","decoder_dense = Dense(tar_vocab_size, activation='softmax') #vocab size만큼의 단어 분포 나오고 거기서 softmax\n","decoder_outputs = decoder_dense(decoder_outputs2) #output을 dense에 넣어줌\n","\n","# 모델의 입력과 출력을 정의.\n","model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n","\n","model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])\n","#sparse cross entropy는 결과 값을 one-hot coding하지 않았을 때 cross entropy를 구해주는 loss function"],"metadata":{"id":"yOXL9XES9QUJ","executionInfo":{"status":"ok","timestamp":1648800583458,"user_tz":-540,"elapsed":2073,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tl769iR5boCw","executionInfo":{"status":"ok","timestamp":1648800583458,"user_tz":-540,"elapsed":21,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}},"outputId":"059d069e-8dec-4d56-99a8-86f7e0a0fd9b"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_1 (InputLayer)           [(None, None)]       0           []                               \n","                                                                                                  \n"," input_2 (InputLayer)           [(None, None)]       0           []                               \n","                                                                                                  \n"," embedding (Embedding)          (None, None, 64)     335360      ['input_1[0][0]']                \n","                                                                                                  \n"," embedding_1 (Embedding)        (None, None, 64)     579072      ['input_2[0][0]']                \n","                                                                                                  \n"," masking (Masking)              (None, None, 64)     0           ['embedding[0][0]']              \n","                                                                                                  \n"," masking_1 (Masking)            (None, None, 64)     0           ['embedding_1[0][0]']            \n","                                                                                                  \n"," lstm (LSTM)                    [(None, None, 20),   6800        ['masking[0][0]']                \n","                                 (None, 20),                                                      \n","                                 (None, 20)]                                                      \n","                                                                                                  \n"," lstm_2 (LSTM)                  [(None, None, 20),   6800        ['masking_1[0][0]',              \n","                                 (None, 20),                      'lstm[0][1]',                   \n","                                 (None, 20)]                      'lstm[0][2]']                   \n","                                                                                                  \n"," lstm_1 (LSTM)                  [(None, 20),         3280        ['lstm[0][0]']                   \n","                                 (None, 20),                                                      \n","                                 (None, 20)]                                                      \n","                                                                                                  \n"," lstm_3 (LSTM)                  [(None, None, 20),   3280        ['lstm_2[0][0]',                 \n","                                 (None, 20),                      'lstm_1[0][1]',                 \n","                                 (None, 20)]                      'lstm_1[0][2]']                 \n","                                                                                                  \n"," dense (Dense)                  (None, None, 9048)   190008      ['lstm_3[0][0]']                 \n","                                                                                                  \n","==================================================================================================\n","Total params: 1,124,600\n","Trainable params: 1,124,600\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"]}]},{"cell_type":"code","source":["model.fit(x=[encoder_input_train, decoder_input_train], y=decoder_target_train, \\\n","          validation_data=([encoder_input_test, decoder_input_test], decoder_target_test),\n","          batch_size=128, epochs=50)"],"metadata":{"id":"vLmDMNJiAfw7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648801311155,"user_tz":-540,"elapsed":727704,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}},"outputId":"a5f97f87-f02d-4acb-a71d-d954d40640ff"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n","282/282 [==============================] - 45s 80ms/step - loss: 4.8270 - acc: 0.6026 - val_loss: 2.6235 - val_acc: 0.6071\n","Epoch 2/50\n","282/282 [==============================] - 14s 49ms/step - loss: 2.4007 - acc: 0.6072 - val_loss: 2.1897 - val_acc: 0.6071\n","Epoch 3/50\n","282/282 [==============================] - 14s 49ms/step - loss: 2.0269 - acc: 0.6367 - val_loss: 1.8988 - val_acc: 0.7107\n","Epoch 4/50\n","282/282 [==============================] - 14s 49ms/step - loss: 1.7930 - acc: 0.7331 - val_loss: 1.7239 - val_acc: 0.7391\n","Epoch 5/50\n","282/282 [==============================] - 14s 49ms/step - loss: 1.6547 - acc: 0.7415 - val_loss: 1.6247 - val_acc: 0.7435\n","Epoch 6/50\n","282/282 [==============================] - 14s 49ms/step - loss: 1.5683 - acc: 0.7483 - val_loss: 1.5574 - val_acc: 0.7508\n","Epoch 7/50\n","282/282 [==============================] - 14s 49ms/step - loss: 1.5047 - acc: 0.7545 - val_loss: 1.5045 - val_acc: 0.7561\n","Epoch 8/50\n","282/282 [==============================] - 14s 49ms/step - loss: 1.4506 - acc: 0.7596 - val_loss: 1.4607 - val_acc: 0.7604\n","Epoch 9/50\n","282/282 [==============================] - 14s 49ms/step - loss: 1.4051 - acc: 0.7643 - val_loss: 1.4247 - val_acc: 0.7634\n","Epoch 10/50\n","282/282 [==============================] - 14s 49ms/step - loss: 1.3612 - acc: 0.7716 - val_loss: 1.3850 - val_acc: 0.7754\n","Epoch 11/50\n","282/282 [==============================] - 14s 49ms/step - loss: 1.3205 - acc: 0.7821 - val_loss: 1.3577 - val_acc: 0.7795\n","Epoch 12/50\n","282/282 [==============================] - 14s 49ms/step - loss: 1.2880 - acc: 0.7866 - val_loss: 1.3335 - val_acc: 0.7825\n","Epoch 13/50\n","282/282 [==============================] - 14s 50ms/step - loss: 1.2599 - acc: 0.7897 - val_loss: 1.3119 - val_acc: 0.7852\n","Epoch 14/50\n","282/282 [==============================] - 14s 49ms/step - loss: 1.2351 - acc: 0.7929 - val_loss: 1.2938 - val_acc: 0.7882\n","Epoch 15/50\n","282/282 [==============================] - 14s 49ms/step - loss: 1.2115 - acc: 0.7969 - val_loss: 1.2756 - val_acc: 0.7916\n","Epoch 16/50\n","282/282 [==============================] - 14s 49ms/step - loss: 1.1886 - acc: 0.8010 - val_loss: 1.2588 - val_acc: 0.7941\n","Epoch 17/50\n","282/282 [==============================] - 14s 50ms/step - loss: 1.1678 - acc: 0.8030 - val_loss: 1.2453 - val_acc: 0.7959\n","Epoch 18/50\n","282/282 [==============================] - 14s 50ms/step - loss: 1.1475 - acc: 0.8054 - val_loss: 1.2311 - val_acc: 0.7978\n","Epoch 19/50\n","282/282 [==============================] - 14s 49ms/step - loss: 1.1277 - acc: 0.8073 - val_loss: 1.2178 - val_acc: 0.7986\n","Epoch 20/50\n","282/282 [==============================] - 14s 49ms/step - loss: 1.1085 - acc: 0.8097 - val_loss: 1.2033 - val_acc: 0.8009\n","Epoch 21/50\n","282/282 [==============================] - 14s 49ms/step - loss: 1.0890 - acc: 0.8117 - val_loss: 1.1924 - val_acc: 0.8021\n","Epoch 22/50\n","282/282 [==============================] - 14s 49ms/step - loss: 1.0703 - acc: 0.8137 - val_loss: 1.1798 - val_acc: 0.8028\n","Epoch 23/50\n","282/282 [==============================] - 14s 49ms/step - loss: 1.0528 - acc: 0.8157 - val_loss: 1.1668 - val_acc: 0.8045\n","Epoch 24/50\n","282/282 [==============================] - 14s 49ms/step - loss: 1.0350 - acc: 0.8183 - val_loss: 1.1547 - val_acc: 0.8070\n","Epoch 25/50\n","282/282 [==============================] - 14s 50ms/step - loss: 1.0188 - acc: 0.8204 - val_loss: 1.1449 - val_acc: 0.8080\n","Epoch 26/50\n","282/282 [==============================] - 14s 49ms/step - loss: 1.0031 - acc: 0.8223 - val_loss: 1.1343 - val_acc: 0.8089\n","Epoch 27/50\n","282/282 [==============================] - 14s 49ms/step - loss: 0.9874 - acc: 0.8240 - val_loss: 1.1238 - val_acc: 0.8109\n","Epoch 28/50\n","282/282 [==============================] - 14s 49ms/step - loss: 0.9729 - acc: 0.8257 - val_loss: 1.1173 - val_acc: 0.8109\n","Epoch 29/50\n","282/282 [==============================] - 14s 50ms/step - loss: 0.9594 - acc: 0.8272 - val_loss: 1.1065 - val_acc: 0.8139\n","Epoch 30/50\n","282/282 [==============================] - 14s 50ms/step - loss: 0.9463 - acc: 0.8293 - val_loss: 1.1008 - val_acc: 0.8147\n","Epoch 31/50\n","282/282 [==============================] - 14s 49ms/step - loss: 0.9336 - acc: 0.8309 - val_loss: 1.0950 - val_acc: 0.8158\n","Epoch 32/50\n","282/282 [==============================] - 14s 49ms/step - loss: 0.9221 - acc: 0.8321 - val_loss: 1.0860 - val_acc: 0.8168\n","Epoch 33/50\n","282/282 [==============================] - 14s 50ms/step - loss: 0.9105 - acc: 0.8335 - val_loss: 1.0825 - val_acc: 0.8176\n","Epoch 34/50\n","282/282 [==============================] - 14s 50ms/step - loss: 0.8994 - acc: 0.8349 - val_loss: 1.0776 - val_acc: 0.8194\n","Epoch 35/50\n","282/282 [==============================] - 14s 48ms/step - loss: 0.8893 - acc: 0.8362 - val_loss: 1.0729 - val_acc: 0.8183\n","Epoch 36/50\n","282/282 [==============================] - 14s 49ms/step - loss: 0.8790 - acc: 0.8372 - val_loss: 1.0676 - val_acc: 0.8203\n","Epoch 37/50\n","282/282 [==============================] - 14s 49ms/step - loss: 0.8694 - acc: 0.8382 - val_loss: 1.0640 - val_acc: 0.8210\n","Epoch 38/50\n","282/282 [==============================] - 14s 50ms/step - loss: 0.8599 - acc: 0.8394 - val_loss: 1.0601 - val_acc: 0.8213\n","Epoch 39/50\n","282/282 [==============================] - 15s 53ms/step - loss: 0.8510 - acc: 0.8403 - val_loss: 1.0563 - val_acc: 0.8227\n","Epoch 40/50\n","282/282 [==============================] - 14s 51ms/step - loss: 0.8417 - acc: 0.8415 - val_loss: 1.0546 - val_acc: 0.8226\n","Epoch 41/50\n","282/282 [==============================] - 15s 52ms/step - loss: 0.8334 - acc: 0.8422 - val_loss: 1.0500 - val_acc: 0.8232\n","Epoch 42/50\n","282/282 [==============================] - 14s 49ms/step - loss: 0.8248 - acc: 0.8434 - val_loss: 1.0471 - val_acc: 0.8240\n","Epoch 43/50\n","282/282 [==============================] - 14s 49ms/step - loss: 0.8166 - acc: 0.8443 - val_loss: 1.0431 - val_acc: 0.8246\n","Epoch 44/50\n","282/282 [==============================] - 14s 49ms/step - loss: 0.8091 - acc: 0.8454 - val_loss: 1.0415 - val_acc: 0.8248\n","Epoch 45/50\n","282/282 [==============================] - 14s 49ms/step - loss: 0.8008 - acc: 0.8464 - val_loss: 1.0384 - val_acc: 0.8258\n","Epoch 46/50\n","282/282 [==============================] - 14s 50ms/step - loss: 0.7936 - acc: 0.8471 - val_loss: 1.0369 - val_acc: 0.8259\n","Epoch 47/50\n","282/282 [==============================] - 14s 49ms/step - loss: 0.7862 - acc: 0.8482 - val_loss: 1.0386 - val_acc: 0.8250\n","Epoch 48/50\n","282/282 [==============================] - 14s 49ms/step - loss: 0.7797 - acc: 0.8488 - val_loss: 1.0333 - val_acc: 0.8265\n","Epoch 49/50\n","282/282 [==============================] - 14s 50ms/step - loss: 0.7725 - acc: 0.8497 - val_loss: 1.0326 - val_acc: 0.8269\n","Epoch 50/50\n","282/282 [==============================] - 14s 49ms/step - loss: 0.7653 - acc: 0.8506 - val_loss: 1.0298 - val_acc: 0.8274\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f23d01ad4d0>"]},"metadata":{},"execution_count":19}]},{"cell_type":"markdown","source":["## 학습 이후 학습 된 모델로 machine translation 실행"],"metadata":{"id":"-abSS9tRBg5S"}},{"cell_type":"code","source":["# 인코더는 train에서 사용되고 이미 train된 애들을 그대로 가져옵니다\n","encoder_model = Model(encoder_inputs, [encoder_states1]+[encoder_states2])\n","\n","# 디코더 설계 시작\n","# 이전 시점의 상태를 보관할 텐서\n","decoder_state_input_h1 = Input(shape=(hidden_units,))\n","decoder_state_input_c1 = Input(shape=(hidden_units,))\n","decoder_states_inputs1 = [decoder_state_input_h1, decoder_state_input_c1]\n","\n","decoder_state_input_h2 = Input(shape=(hidden_units,))\n","decoder_state_input_c2 = Input(shape=(hidden_units,))\n","decoder_states_inputs2 = [decoder_state_input_h2, decoder_state_input_c2]\n","\n","# 훈련 때 사용했던 임베딩 층을 재사용\n","dec_emb2 = dec_emb_layer(decoder_inputs)\n","\n","# 첫 번째 lstm\n","dec_outputs1, state_h1, state_c1 = decoder_lstm1(dec_emb2, initial_state=decoder_states_inputs1)\n","dec_states1 = [state_h1, state_c1]\n","\n","#두 번째 lstm\n","dec_outputs2, state_h2, state_c2 = decoder_lstm2(dec_outputs1, initial_state=decoder_states_inputs2)\n","dec_states2 = [state_h2, state_c2]\n","\n","# 모든 시점에 대해서 단어 예측\n","dec_outputs2 = decoder_dense(dec_outputs2) #output을 desne에 넣어서 확률 처리\n","\n","# 수정된 디코더\n","decoder_model = Model(\n","    [decoder_inputs] + decoder_states_inputs1+decoder_states_inputs2,[dec_outputs1]+dec_states1 + [dec_outputs2] + dec_states2)"],"metadata":{"id":"qOBVmsreBlQ3","executionInfo":{"status":"ok","timestamp":1648808274072,"user_tz":-540,"elapsed":1009,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}}},"execution_count":111,"outputs":[]},{"cell_type":"code","source":["#디코더를 컨트롤하기 위한 함수, test과정에서 사용할 함수\n","\n","def decode_sequence(input_seq):\n","  # 입력으로부터 인코더의 마지막 시점의 상태(은닉 상태, 셀 상태)를 얻음\n","  states_value = encoder_model.predict(input_seq)\n","\n","  # <SOS>에 해당하는 정수 생성\n","  target_seq = np.zeros((1,1))\n","  target_seq[0, 0] = tar_to_index['<sos>'] #시작토큰의 inex를 시작에 넣어줌\n","\n","  stop_condition = False\n","  decoded_sentence = ''\n","\n","  # stop_condition이 True가 될 때까지 루프 반복\n","  # 구현의 간소화를 위해서 이 함수는 배치 크기를 1로 가정합니다.\n","  while not stop_condition:\n","    # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n","    out,h1,c1,output_tokens,h2, c2 = decoder_model.predict([target_seq] + states_value) \n","\n","    # 예측 결과를 단어로 변환\n","    sampled_token_index = np.argmax(output_tokens[0, -1, :])\n","    sampled_char = index_to_tar[sampled_token_index]\n","\n","    # 현재 시점의 예측 단어를 예측 문장에 추가\n","    decoded_sentence += ' '+sampled_char\n","\n","    # <eos>에 도달하거나 정해진 길이를 넘으면 중단.\n","    if (sampled_char == '<eos>' or\n","        len(decoded_sentence) > 50):\n","        stop_condition = True\n","\n","    # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n","    target_seq = np.zeros((1,1))\n","    target_seq[0, 0] = sampled_token_index\n","\n","    # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n","    states_value = [h1, c1, h2, c2]\n","\n","  return decoded_sentence"],"metadata":{"id":"ns9zUIDkkpu1","executionInfo":{"status":"ok","timestamp":1648808277237,"user_tz":-540,"elapsed":528,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}}},"execution_count":112,"outputs":[]},{"cell_type":"code","source":["# 원문의 정수 시퀀스를 텍스트 시퀀스로 변환\n","def seq_to_src(input_seq):\n","  sentence = ''\n","  for encoded_word in input_seq:\n","    if(encoded_word != 0):\n","      sentence = sentence + index_to_src[encoded_word] + ' '\n","  return sentence\n","\n","# 번역문의 정수 시퀀스를 텍스트 시퀀스로 변환\n","def seq_to_tar(input_seq):\n","  sentence = ''\n","  for encoded_word in input_seq:\n","    if(encoded_word != 0 and encoded_word != tar_to_index['<sos>'] and encoded_word != tar_to_index['<eos>']):\n","      sentence = sentence + index_to_tar[encoded_word] + ' '\n","  return sentence"],"metadata":{"id":"b2CA_lYdZssM","executionInfo":{"status":"ok","timestamp":1648808278839,"user_tz":-540,"elapsed":3,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}}},"execution_count":113,"outputs":[]},{"cell_type":"code","source":["for seq_index in [6,3, 50, 100, 300, 1001]:\n","  input_seq = encoder_input_train[seq_index: seq_index + 1]\n","  decoded_sentence = decode_sequence(input_seq)\n","  print(\"입력문장 :\",seq_to_src(encoder_input_train[seq_index]))\n","  print(\"정답문장 :\",seq_to_tar(decoder_input_train[seq_index]))\n","  print(\"번역문장 :\",decoded_sentence[1:-5])\n","  print('BLEU :',bleu.sentence_bleu(list(map(lambda ref: ref.split(), seq_to_tar(decoder_input_train[seq_index]))),decoded_sentence[1:-5].split()))\n","  print(\"-\"*50)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Gc_5xnbXaXmT","executionInfo":{"status":"ok","timestamp":1648808287898,"user_tz":-540,"elapsed":7392,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}},"outputId":"3e6eb4ed-0ed4-4114-c777-5034603c3b98"},"execution_count":114,"outputs":[{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:6 out of the last 12 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f232768ab90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","입력문장 : he lost everything . \n","정답문장 : il a tout perdu . \n","번역문장 : il etait des excuses . \n","BLEU : 0.668740304976422\n","--------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n","Corpus/Sentence contains 0 counts of 2-gram overlaps.\n","BLEU scores might be undesirable; use SmoothingFunction().\n","  warnings.warn(_msg)\n"]},{"output_type":"stream","name":"stdout","text":["입력문장 : tell us a joke . \n","정답문장 : raconte nous une blague . \n","번역문장 : veuillez vous soyez bon ! \n","BLEU : 0\n","--------------------------------------------------\n","입력문장 : i d never hire tom . \n","정답문장 : je n engagerais jamais tom . \n","번역문장 : je ne peux pas a l interieur . \n","BLEU : 0.7071067811865476\n","--------------------------------------------------\n","입력문장 : no one likes me . \n","정답문장 : personne ne m apprecie . \n","번역문장 : personne ne peut le francais . \n","BLEU : 0.6389431042462724\n","--------------------------------------------------\n","입력문장 : you re weak . \n","정답문장 : vous etes faible . \n","번역문장 : tu es fort . \n","BLEU : 0.7071067811865476\n","--------------------------------------------------\n","입력문장 : i m very lazy . \n","정답문장 : je suis tres paresseux . \n","번역문장 : je suis tres occupee . \n","BLEU : 0.668740304976422\n","--------------------------------------------------\n"]}]},{"cell_type":"code","source":["for seq_index in [3, 50, 100, 300, 1001]:\n","  input_seq = encoder_input_test[seq_index: seq_index + 1]\n","  decoded_sentence = decode_sequence(input_seq)\n","\n","  print(\"입력문장 :\",seq_to_src(encoder_input_test[seq_index]))\n","  print(\"정답문장 :\",seq_to_tar(decoder_input_test[seq_index]))\n","  print(\"번역문장 :\",decoded_sentence[1:-5])\n","  print('BLEU :',bleu.sentence_bleu(list(map(lambda ref: ref.split(), seq_to_tar(decoder_input_test[seq_index]))),decoded_sentence[1:-5].split()))\n","\n","  print(\"-\"*50)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DOGEwKWFdMwt","executionInfo":{"status":"ok","timestamp":1648808333322,"user_tz":-540,"elapsed":2773,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}},"outputId":"dc97db28-15c3-48a0-9f2b-e60a9d77c93f"},"execution_count":115,"outputs":[{"output_type":"stream","name":"stdout","text":["입력문장 : tom grinned . \n","정답문장 : tom souriait . \n","번역문장 : tom a des jours . \n","BLEU : 0.7952707287670506\n","--------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n","Corpus/Sentence contains 0 counts of 2-gram overlaps.\n","BLEU scores might be undesirable; use SmoothingFunction().\n","  warnings.warn(_msg)\n"]},{"output_type":"stream","name":"stdout","text":["입력문장 : i rewrote it . \n","정답문장 : je le reecrivis . \n","번역문장 : je l ai fait . \n","BLEU : 0.7952707287670506\n","--------------------------------------------------\n","입력문장 : turn right there . \n","정답문장 : tournez la devant a droite . \n","번역문장 : salut ca . \n","BLEU : 0.7598356856515925\n","--------------------------------------------------\n","입력문장 : no one can stop me . \n","정답문장 : personne ne peut m arreter . \n","번역문장 : personne ne peut etre faire . \n","BLEU : 0.6389431042462724\n","--------------------------------------------------\n","입력문장 : what was that thing ? \n","정답문장 : qu etait ce ? \n","번역문장 : qu est ce que il a la maison ? \n","BLEU : 0.6865890479690392\n","--------------------------------------------------\n"]}]}]}