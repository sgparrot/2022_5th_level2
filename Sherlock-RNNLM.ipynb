{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Sherlock-RNNLM.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP8x6PY8BnzgXQzCUHfyCqa"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"gRHAP0owMeZl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648568180663,"user_tz":-540,"elapsed":13131,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}},"outputId":"e17eaed0-e747-4add-cf59-af06f8a3c67b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: glove_python_binary in /usr/local/lib/python3.7/dist-packages (0.2.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from glove_python_binary) (1.4.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from glove_python_binary) (1.21.5)\n"]}],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib as plt\n","\n","from collections import Counter\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.layers import Embedding, Dense, SimpleRNN\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.optimizers import Adam\n","\n","import re\n","from nltk.corpus import stopwords\n","from keras.callbacks import EarlyStopping, ModelCheckpoint\n","from tensorflow.keras.preprocessing.text import text_to_word_sequence\n","from gensim.models.keyedvectors import KeyedVectors\n","!pip install glove_python_binary\n","from glove import Corpus, Glove\n"]},{"cell_type":"code","source":["f = open(\"/content/input.txt\")\n","dataset = f.readlines()\n","f.close\n","print(len(dataset))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bv-Nzi5ZMtcU","executionInfo":{"status":"ok","timestamp":1648568180666,"user_tz":-540,"elapsed":43,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}},"outputId":"36353012-5d6f-4151-d287-88f21f8e7995"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["40000\n"]}]},{"cell_type":"code","source":["#간단한 전처리\n","data = []\n","def preprocess(text):\n","  text = re.sub(r\"[^A-Za-z ]\",\"\", text) # 영어,한글만 포함\n","  return text\n","for i in range(40000):\n","  if preprocess(dataset[i]) not in [\"First Citizen\",\"All\",\"Seocond Citizen\",\"MENENIUS\",\"COMINIUS\",\"MARCIUS\",\"AUFIDIUS\",\"Fisrt Soldier\",\"BRUTUS\",\"\",\"SICINIUS\",\"Both\",\"VOLUMNIA\",\"VIRGILIA\",\"VALERIA\"]:\n","    data.append(preprocess(dataset[i].lower()))"],"metadata":{"id":"OAhbazvwgmpQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##토큰화\n","단어들을 정수로 인코딩하는 과정"],"metadata":{"id":"Ff59LKpqIvd9"}},{"cell_type":"code","source":["tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(data)\n","counts=tokenizer.word_counts #단어의 발생 빈도\n","word_index = tokenizer.word_index #단어의 index를 dictionary로 저장 이후 padding과 wordembedding에 사용\n","tokens=tokenizer.texts_to_sequences(data) #문자를 정수들의 sequence로 반환"],"metadata":{"id":"a-L-_3EtmLZd","executionInfo":{"status":"ok","timestamp":1648568182050,"user_tz":-540,"elapsed":1420,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["#단어 종류의 수\n","vocab_size=0\n","for index,word in enumerate(word_index):\n","  vocab_size=max(vocab_size,index)\n","print(vocab_size)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lLLuNodE5JAq","executionInfo":{"status":"ok","timestamp":1648568182053,"user_tz":-540,"elapsed":44,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}},"outputId":"b7dfd937-dd01-471b-bd00-4623ca20a140"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["12846\n"]}]},{"cell_type":"code","source":["#한 문장에 최대 몇개의 단어가 있는가?\n","max_len =0\n","count=0\n","for i in (tokens):\n","  max_len =max(max_len,len(i))\n","print(max_len)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AxLR8xlxwMxe","executionInfo":{"status":"ok","timestamp":1648568182056,"user_tz":-540,"elapsed":40,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}},"outputId":"193b70c1-845b-4e9c-bda9-00c99b7c595a"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["16\n"]}]},{"cell_type":"markdown","source":["##Glove을 직접 학습 시켜 embedding 계층에 사용"],"metadata":{"id":"TH4feQD9RvyZ"}},{"cell_type":"code","source":["word_list=[]\n","for i in range(len(data)):\n","  word_list.append(data[i].split())"],"metadata":{"id":"_yI2FdObRKM5","executionInfo":{"status":"ok","timestamp":1648568182059,"user_tz":-540,"elapsed":33,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["corpus = Corpus() \n","\n","# 훈련 데이터로부터 GloVe에서 사용할 동시 등장 행렬 생성\n","corpus.fit(word_list, window=10)\n","glove = Glove(no_components=300, learning_rate=0.05)\n","\n","# 학습에 이용할 쓰레드의 개수는 4로 설정, 에포크는 20.\n","glove.fit(corpus.matrix, epochs=20, no_threads=4, verbose=True)\n","glove.add_dictionary(corpus.dictionary)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kSX8uGn8RN42","executionInfo":{"status":"ok","timestamp":1648568230747,"user_tz":-540,"elapsed":48328,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}},"outputId":"757d5a4a-75a7-4768-da9e-c50abe506dff"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Performing 20 training epochs with 4 threads\n","Epoch 0\n","Epoch 1\n","Epoch 2\n","Epoch 3\n","Epoch 4\n","Epoch 5\n","Epoch 6\n","Epoch 7\n","Epoch 8\n","Epoch 9\n","Epoch 10\n","Epoch 11\n","Epoch 12\n","Epoch 13\n","Epoch 14\n","Epoch 15\n","Epoch 16\n","Epoch 17\n","Epoch 18\n","Epoch 19\n"]}]},{"cell_type":"code","source":["#위에서 학습한 glove vector를 embedding에 넣기위해서 embedding_matrix에 저장\n","embedding_matrix=np.zeros((vocab_size,300))\n","for index, word in enumerate(word_index): #vocabulary에 있는 토큰들을 하나씩 넘겨줍니다.\n","      embedding_matrix[index-1] = glove.word_vectors[index-1] #해당 위치의 embedding_mxtrix에 저장합니다."],"metadata":{"id":"ZJkOWwuTR0Un","executionInfo":{"status":"ok","timestamp":1648568442659,"user_tz":-540,"elapsed":281,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["##X,Y dataset 만들기"],"metadata":{"id":"cjdBC02cTR49"}},{"cell_type":"code","source":["sequences = pad_sequences(tokens, maxlen = max_len,padding='pre') \n","#padding하는 과정 padding이란 input으로 사용하기 위해 길이를 맞춰주는 과정"],"metadata":{"id":"sxd2emXwEh8C","executionInfo":{"status":"ok","timestamp":1648568230748,"user_tz":-540,"elapsed":46,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["sequences = np.array(sequences) \n","X = sequences[:,:-1]\n","\n","\n","#Y = sequences[:,1:] #many-to-many용)\n","\n","#Y = sequences[:,-1] #many-to-one용"],"metadata":{"id":"7M8orQABFuTL","executionInfo":{"status":"ok","timestamp":1648568414735,"user_tz":-540,"elapsed":317,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["print(sequences[0],X[0],Y[0]) "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"999C9jfYFvL9","executionInfo":{"status":"ok","timestamp":1648568148789,"user_tz":-540,"elapsed":350,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}},"outputId":"5aabe92a-9b50-4eb4-94d2-0be4959f7765"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["[  0   0   0   0   0   0   0   0 137  35 961 141 657 124  15 104] [  0   0   0   0   0   0   0   0 137  35 961 141 657 124  15] [  0   0   0   0   0   0   0 137  35 961 141 657 124  15 104]\n"]}]},{"cell_type":"code","source":["Y = to_categorical(Y, num_classes=vocab_size+2) \n","#Y는 cross_entropy를 위해서 one-hot vector로 수정"],"metadata":{"id":"H_6KuY1IISBR","executionInfo":{"status":"ok","timestamp":1648568422121,"user_tz":-540,"elapsed":1184,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["##pre_trained glove matrix를 가져와서 embedding 에 적용\n","직접 학습시키거나 이미 학습된걸 가져오거나 선택"],"metadata":{"id":"aIJQOLe8JWHB"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount(\"/content/drive\")"],"metadata":{"id":"sQKk2iyt_om0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648532687154,"user_tz":-540,"elapsed":2145,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}},"outputId":"2a842580-a307-4d2e-9f42-0dbc9a759d09"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["!unzip -uq \"/content/drive/MyDrive/glove.6B.zip\" -d \"/content\""],"metadata":{"id":"oxZ_6B8kFFz3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["glove = dict()\n","\n","f = open('glove.6B.300d.txt', encoding=\"utf8\")\n","\n","for line in f:\n","    word_vector = line.split()\n","    word = word_vector[0]\n","\n","    word_vector_arr = np.asarray(word_vector[1:], dtype='float32')\n","    glove[word] = word_vector_arr\n","f.close()"],"metadata":{"id":"JHFLbXsnFJ5V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#내가 쓸 단어 리스트가 glove에 있으면 그 word2vec가져오고 나머지는 0이상의 작은 소수로 초기화\n","word_list=glove.keys()\n","embedding_matrix = np.random.rand(vocab_size,300)\n","count=0\n","for index, word in enumerate(word_index): #vocabulary에 있는 토큰들을 하나씩 넘겨줍니다.\n","    if word in word_list: #넘겨 받은 토큰이 word2vec에 존재하면(이미 훈련이 된 토큰이라는 뜻)\n","        embedding_vector = glove[word] #해당 토큰에 해당하는 vector를 불러오고\n","        embedding_matrix[index] = embedding_vector #해당 위치의 embedding_mxtrix에 저장합니다.\n","    else:\n","       count+=1 \n","print(count)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fesBy8wJFSZ8","executionInfo":{"status":"ok","timestamp":1648533080925,"user_tz":-540,"elapsed":288,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}},"outputId":"dbc19a76-1a7e-47aa-e230-d4522e10bbf7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2811\n"]}]},{"cell_type":"code","source":["embedding_size = 300 #word vector의 embedding\n","Hidden = 256 #hidden state의 차원\n","model = Sequential()\n","model.add(Embedding(vocab_size, embedding_size, input_length = max_len-1,weights=[embedding_matrix],trainable=False)) \n","#trainable = false는 이미 학습된 word2vec을 사용하므로 굳이 학습하지 않음\n","model.add(SimpleRNN(Hidden))\n","model.add(Dense(vocab_size+2, activation = 'softmax'))\n","model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')\n","\n","print(model.summary())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TLOGH-v49zfA","executionInfo":{"status":"ok","timestamp":1648569091210,"user_tz":-540,"elapsed":352,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}},"outputId":"09029e5a-4e27-489c-aa86-d0629f03aba6"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_2\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_1 (Embedding)     (None, 15, 300)           3853800   \n","                                                                 \n"," simple_rnn_1 (SimpleRNN)    (None, 256)               142592    \n","                                                                 \n"," dense_1 (Dense)             (None, 12848)             3301936   \n","                                                                 \n","=================================================================\n","Total params: 7,298,328\n","Trainable params: 3,444,528\n","Non-trainable params: 3,853,800\n","_________________________________________________________________\n","None\n"]}]},{"cell_type":"code","source":["es = EarlyStopping(monitor='loss', min_delta=0.001, patience=10, verbose=1, mode='min', restore_best_weights=True)\n","mc = ModelCheckpoint(\"best_model.h5\", monitor='loss', verbose=1, save_weights_only=True, save_best_only=True, mode='min')\n","\n","callbacks = [es, mc]\n","\n","model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","model.fit(X, Y, epochs=20,batch_size = 64, verbose=2,callbacks = callbacks)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fs9jZLmhF_2w","executionInfo":{"status":"ok","timestamp":1648570040696,"user_tz":-540,"elapsed":251617,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}},"outputId":"1735868d-dfd4-4617-b826-6479dbacb719"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","\n","Epoch 1: loss improved from inf to 2.54222, saving model to best_model.h5\n","502/502 - 14s - loss: 2.5422 - accuracy: 0.5058 - 14s/epoch - 27ms/step\n","Epoch 2/20\n","\n","Epoch 2: loss improved from 2.54222 to 2.25006, saving model to best_model.h5\n","502/502 - 12s - loss: 2.2501 - accuracy: 0.5612 - 12s/epoch - 25ms/step\n","Epoch 3/20\n","\n","Epoch 3: loss improved from 2.25006 to 2.12822, saving model to best_model.h5\n","502/502 - 12s - loss: 2.1282 - accuracy: 0.5819 - 12s/epoch - 24ms/step\n","Epoch 4/20\n","\n","Epoch 4: loss improved from 2.12822 to 2.04835, saving model to best_model.h5\n","502/502 - 12s - loss: 2.0483 - accuracy: 0.5944 - 12s/epoch - 25ms/step\n","Epoch 5/20\n","\n","Epoch 5: loss improved from 2.04835 to 1.98244, saving model to best_model.h5\n","502/502 - 12s - loss: 1.9824 - accuracy: 0.6044 - 12s/epoch - 25ms/step\n","Epoch 6/20\n","\n","Epoch 6: loss improved from 1.98244 to 1.91500, saving model to best_model.h5\n","502/502 - 12s - loss: 1.9150 - accuracy: 0.6176 - 12s/epoch - 24ms/step\n","Epoch 7/20\n","\n","Epoch 7: loss improved from 1.91500 to 1.85917, saving model to best_model.h5\n","502/502 - 12s - loss: 1.8592 - accuracy: 0.6277 - 12s/epoch - 24ms/step\n","Epoch 8/20\n","\n","Epoch 8: loss improved from 1.85917 to 1.80324, saving model to best_model.h5\n","502/502 - 12s - loss: 1.8032 - accuracy: 0.6390 - 12s/epoch - 24ms/step\n","Epoch 9/20\n","\n","Epoch 9: loss improved from 1.80324 to 1.75497, saving model to best_model.h5\n","502/502 - 12s - loss: 1.7550 - accuracy: 0.6479 - 12s/epoch - 24ms/step\n","Epoch 10/20\n","\n","Epoch 10: loss improved from 1.75497 to 1.71226, saving model to best_model.h5\n","502/502 - 12s - loss: 1.7123 - accuracy: 0.6566 - 12s/epoch - 24ms/step\n","Epoch 11/20\n","\n","Epoch 11: loss improved from 1.71226 to 1.66865, saving model to best_model.h5\n","502/502 - 12s - loss: 1.6687 - accuracy: 0.6656 - 12s/epoch - 25ms/step\n","Epoch 12/20\n","\n","Epoch 12: loss improved from 1.66865 to 1.62316, saving model to best_model.h5\n","502/502 - 13s - loss: 1.6232 - accuracy: 0.6750 - 13s/epoch - 25ms/step\n","Epoch 13/20\n","\n","Epoch 13: loss improved from 1.62316 to 1.58827, saving model to best_model.h5\n","502/502 - 12s - loss: 1.5883 - accuracy: 0.6785 - 12s/epoch - 25ms/step\n","Epoch 14/20\n","\n","Epoch 14: loss improved from 1.58827 to 1.55249, saving model to best_model.h5\n","502/502 - 12s - loss: 1.5525 - accuracy: 0.6895 - 12s/epoch - 25ms/step\n","Epoch 15/20\n","\n","Epoch 15: loss improved from 1.55249 to 1.52083, saving model to best_model.h5\n","502/502 - 13s - loss: 1.5208 - accuracy: 0.6944 - 13s/epoch - 25ms/step\n","Epoch 16/20\n","\n","Epoch 16: loss improved from 1.52083 to 1.49813, saving model to best_model.h5\n","502/502 - 12s - loss: 1.4981 - accuracy: 0.6980 - 12s/epoch - 25ms/step\n","Epoch 17/20\n","\n","Epoch 17: loss improved from 1.49813 to 1.46616, saving model to best_model.h5\n","502/502 - 12s - loss: 1.4662 - accuracy: 0.7023 - 12s/epoch - 25ms/step\n","Epoch 18/20\n","\n","Epoch 18: loss improved from 1.46616 to 1.43138, saving model to best_model.h5\n","502/502 - 13s - loss: 1.4314 - accuracy: 0.7123 - 13s/epoch - 25ms/step\n","Epoch 19/20\n","\n","Epoch 19: loss improved from 1.43138 to 1.41554, saving model to best_model.h5\n","502/502 - 12s - loss: 1.4155 - accuracy: 0.7148 - 12s/epoch - 25ms/step\n","Epoch 20/20\n","\n","Epoch 20: loss improved from 1.41554 to 1.39370, saving model to best_model.h5\n","502/502 - 14s - loss: 1.3937 - accuracy: 0.7200 - 14s/epoch - 28ms/step\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f6f97b2b090>"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["def sentence_generation(model, tokenizer, current_word, n): # 모델, 토크나이저, 현재 단어, 반복할 횟수\n","    init_word = current_word\n","    sentence = ''\n","\n","    # n번 반복\n","    for _ in range(n):\n","        # 현재 단어에 대한 정수 인코딩과 패딩\n","        encoded = tokenizer.texts_to_sequences([current_word])[0]\n","        encoded = pad_sequences([encoded], maxlen=max_len-1, padding='pre')\n","        # 입력한 X(현재 단어)에 대해서 Y를 예측하고 Y(예측한 단어)를 result에 저장.\n","        result = model.predict(encoded, verbose=0)\n","        result = np.argmax(result, axis=1)\n","\n","        for word, index in tokenizer.word_index.items(): \n","            # 만약 예측한 단어와 인덱스와 동일한 단어가 있다면 break\n","            if index == result:\n","                break\n","\n","        # 현재 단어 + ' ' + 예측 단어를 현재 단어로 변경\n","        current_word = current_word + ' '  + word\n","\n","        # 예측 단어를 문장에 저장\n","        sentence = sentence + ' ' + word\n","\n","    sentence = init_word + sentence\n","    return sentence"],"metadata":{"id":"qOHSQRNNHToT","executionInfo":{"status":"ok","timestamp":1648568779407,"user_tz":-540,"elapsed":5757,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["print(sentence_generation(model, tokenizer, 'not', 10))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0pLdUP-kPXWx","executionInfo":{"status":"ok","timestamp":1648570073026,"user_tz":-540,"elapsed":749,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}},"outputId":"76c76cf3-137e-4f1a-b816-c0a4d03d6896"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["not possible sir traditional son banishment face babes voices well a\n"]}]}]}