{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Seq2Seq_dot_Attention.ipynb의 사본","provenance":[{"file_id":"1K0-D4i5QHJ0Pd9q5skX2bWoHNc1BTMPL","timestamp":1649217044768},{"file_id":"14RlYWyvQWEzG7pOHbOLBPM_wCCMv9oUG","timestamp":1649169183961}],"collapsed_sections":[],"mount_file_id":"14RlYWyvQWEzG7pOHbOLBPM_wCCMv9oUG","authorship_tag":"ABX9TyNgvpQfAzg3JY0Wjy8tjdCp"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"QlNRln8ij7mC","executionInfo":{"status":"ok","timestamp":1649217095158,"user_tz":-540,"elapsed":6015,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}}},"outputs":[],"source":["import os\n","import re\n","\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","import unicodedata\n","import urllib3\n","from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Masking, Concatenate,Attention\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","\n","import nltk.translate.bleu_score as bleu\n","from collections import Counter\n","from nltk import ngrams"]},{"cell_type":"markdown","source":["##학습을 위한 병렬 corpus 가져오기\n","source와 target을 각각 병렬적으로 매치한 데이터셋"],"metadata":{"id":"pIh9-gtZryQQ"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"22XaDPjzkKNE","executionInfo":{"status":"ok","timestamp":1649217114648,"user_tz":-540,"elapsed":16491,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}},"outputId":"6afdbb21-af7e-4a99-dd8a-a72c9af042a8"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!unzip -uq \"/content/drive/MyDrive/fra-eng.zip\" -d \"/content\" \n","\n","#영어와 프랑스어를 매치한 데이터 셋\n","#영어가 source 프랑스어가 target"],"metadata":{"id":"ctZi2Qd0kiz0","executionInfo":{"status":"ok","timestamp":1649217118477,"user_tz":-540,"elapsed":968,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["def to_ascii(s):\n","  # 프랑스어 악센트(accent) 삭제\n","  # 예시 : 'déjà diné' -> deja dine\n","  return ''.join(c for c in unicodedata.normalize('NFD', s)\n","                   if unicodedata.category(c) != 'Mn')\n","\n","def preprocess_sentence(sent):\n","  # 악센트 제거 함수 호출\n","  sent = to_ascii(sent.lower())\n","\n","  # 단어와 구두점 사이에 공백 추가해서 구두점을 구분\n","  # ex) \"I am a student.\" => \"I am a student .\"\n","  sent = re.sub(r\"([?.!,¿])\", r\" \\1\", sent)\n","\n","  # (a-z, A-Z, \".\", \"?\", \"!\", \",\")  영어랑 . ? ! , 제외하고 모두 지움\n","  sent = re.sub(r\"[^a-zA-Z!.?]+\", r\" \", sent)\n","\n","  # 다수 개의 공백을 하나의 공백으로 치환\n","  sent = re.sub(r\"\\s+\", \" \", sent)\n","  return sent"],"metadata":{"id":"ifDy9PUgoUST","executionInfo":{"status":"ok","timestamp":1649217119263,"user_tz":-540,"elapsed":2,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["num_samples =40000 #원래는 19만개의 문장이 있는데 3만개정도만 사용\n","\n","def load_preprocessed_data():\n","  encoder_input, decoder_input, decoder_target = [], [], [] \n","  #input을 3개 만듦 encoder는 output이 없으므로 encoder target은 만들 필요 x\n","  with open(\"fra.txt\", \"r\") as lines:\n","    for i, line in enumerate(lines): #line 하나 안에 tab을 기준으로 source와 target을 구분하고 있음\n","      # source 데이터와 target 데이터를 tab을 기준으로 분리\n","      src_line, tar_line, _ = line.strip().split('\\t')\n","\n","      # source 데이터 전처리\n","      src_line = [w for w in preprocess_sentence(src_line).split()]\n","\n","      # target 데이터 전처리\n","      tar_line = preprocess_sentence(tar_line)\n","      tar_line_in = [w for w in (\"<sos> \" + tar_line).split()] #line을 받아와서 sos 토큰을 넣어준다\n","      tar_line_out = [w for w in (tar_line + \" <eos>\").split()] #line을 받아와서 eos 토큰을 넣어준다.\n","\n","      encoder_input.append(src_line)\n","      decoder_input.append(tar_line_in)\n","      decoder_target.append(tar_line_out)\n","\n","      if i == num_samples - 1:\n","        break\n","  lines.close()\n","  return encoder_input, decoder_input, decoder_target\n","\n","sents_en_in , sents_fra_in, sents_fra_out = load_preprocessed_data()"],"metadata":{"id":"q0ltJWJ3mAwE","executionInfo":{"status":"ok","timestamp":1649217125371,"user_tz":-540,"elapsed":1998,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["print('인코더의 입력 :',sents_en_in[15])\n","print('디코더의 입력 :',sents_fra_in[15])\n","print('디코더의 레이블 :',sents_fra_out[15])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YV53HrgUuJee","executionInfo":{"status":"ok","timestamp":1649217125372,"user_tz":-540,"elapsed":12,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}},"outputId":"4790b373-955c-4f80-caf0-eb757df1fc40"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["인코더의 입력 : ['run', '.']\n","디코더의 입력 : ['<sos>', 'prenez', 'vos', 'jambes', 'a', 'vos', 'cous', '!']\n","디코더의 레이블 : ['prenez', 'vos', 'jambes', 'a', 'vos', 'cous', '!', '<eos>']\n"]}]},{"cell_type":"markdown","source":["##Seq2Seq에 넣기 위해서 tokenize\n","\n","영어와 프랑스어 각각에 대해서 tokenize함\n"],"metadata":{"id":"LBBnRubLwq7M"}},{"cell_type":"code","source":["#tokenizer 안에 filters는 문장 내에서 \"\"안에 것을 filtering함 우리는 이미 전처리 다해놔서 거를 것 없음 lower도 이미 해놓음\n","\n","tokenizer_en = Tokenizer(filters=\"\", lower=False)\n","tokenizer_en.fit_on_texts(sents_en_in)\n","\n","tokenizer_fra = Tokenizer(filters=\"\", lower=False)\n","tokenizer_fra.fit_on_texts(sents_fra_in)\n","tokenizer_fra.fit_on_texts(sents_fra_out)\n","\n","\n","encoder_input = tokenizer_en.texts_to_sequences(sents_en_in)\n","encoder_input = pad_sequences(encoder_input, padding=\"post\")\n","#padding에는 2가지 pre, post 존재 post는 0들을 뒤에 채우는 것\n","\n","decoder_input = tokenizer_fra.texts_to_sequences(sents_fra_in)\n","decoder_input = pad_sequences(decoder_input, padding=\"post\")\n","\n","decoder_target = tokenizer_fra.texts_to_sequences(sents_fra_out)\n","decoder_target = pad_sequences(decoder_target, padding=\"post\")"],"metadata":{"id":"XP8PCJ5FvuPT","executionInfo":{"status":"error","timestamp":1649217668586,"user_tz":-540,"elapsed":1180,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}},"colab":{"base_uri":"https://localhost:8080/","height":397},"outputId":"b514ffe0-e021-4d26-e354-0c5f1fedf78b"},"execution_count":22,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-c83c538d81d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mencoder_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer_en\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msents_en_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mencoder_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;31m#padding에는 2가지 pre, post 존재 post는 0들을 뒤에 채우는 것\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/preprocessing/sequence.py\u001b[0m in \u001b[0;36mpad_sequences\u001b[0;34m(sequences, maxlen, dtype, padding, truncating, value)\u001b[0m\n\u001b[1;32m    152\u001b[0m   return sequence.pad_sequences(\n\u001b[1;32m    153\u001b[0m       \u001b[0msequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m       padding=padding, truncating=truncating, value=value)\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m keras_export(\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras_preprocessing/sequence.py\u001b[0m in \u001b[0;36mpad_sequences\u001b[0;34m(sequences, maxlen, dtype, padding, truncating, value)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \"\"\"\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'`sequences` must be iterable.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0mnum_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: `sequences` must be iterable."]}]},{"cell_type":"code","source":["encoder_input[0]"],"metadata":{"id":"mjUVaS5ixqe5","executionInfo":{"status":"ok","timestamp":1649217128874,"user_tz":-540,"elapsed":8,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"317d0a79-a4d9-48ab-a927-089077c06255"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([31,  1,  0,  0,  0,  0,  0,  0], dtype=int32)"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["#영어는 문장 최대 길이가 8인 반면 프랑스어는 최대 길이가 16\n","\n","print('인코더의 입력의 크기(shape) :',encoder_input.shape)\n","print('디코더의 입력의 크기(shape) :',decoder_input.shape)\n","print('디코더의 레이블의 크기(shape) :',decoder_target.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Gj0X4fGnx3cP","executionInfo":{"status":"ok","timestamp":1649217129246,"user_tz":-540,"elapsed":5,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}},"outputId":"ccad92cb-5510-4501-e670-56e68b1fd582"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["인코더의 입력의 크기(shape) : (40000, 8)\n","디코더의 입력의 크기(shape) : (40000, 16)\n","디코더의 레이블의 크기(shape) : (40000, 16)\n"]}]},{"cell_type":"code","source":["#word_index는 각 단어에 대한 index를 매칭해서 dictionary로 반환\n","src_vocab_size = len(tokenizer_en.word_index) + 1\n","tar_vocab_size = len(tokenizer_fra.word_index) + 1\n","print(\"영어 단어 집합의 크기 : {:d}, 프랑스어 단어 집합의 크기 : {:d}\".format(src_vocab_size, tar_vocab_size))"],"metadata":{"id":"3qUWf1Pcx9y2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649217130167,"user_tz":-540,"elapsed":3,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}},"outputId":"c09cb2d5-d453-4f90-a163-26fe9574c88e"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["영어 단어 집합의 크기 : 5240, 프랑스어 단어 집합의 크기 : 9048\n"]}]},{"cell_type":"code","source":["#word_index는 단어 - 인덱스 순의 dictionary\n","#index_word는 그 반대 아래의 딕셔너리는 이후 예측값과 실제값 예측에 사용\n","\n","src_to_index = tokenizer_en.word_index\n","index_to_src = tokenizer_en.index_word\n","tar_to_index = tokenizer_fra.word_index\n","index_to_tar = tokenizer_fra.index_word"],"metadata":{"id":"QQI6x7IOyJ9t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#33000개의 문장을 무작위로 shuffle\n","indices = np.arange(num_samples)\n","np.random.shuffle(indices)\n","encoder_input = encoder_input[indices]\n","decoder_input = decoder_input[indices]\n","decoder_target = decoder_target[indices]"],"metadata":{"id":"wfbz9UKPyy0Z","executionInfo":{"status":"ok","timestamp":1649217132061,"user_tz":-540,"elapsed":12,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["val_num=int(0.1*num_samples) #10% 만큼 test에 사용 split 해준다\n","encoder_input_train = encoder_input[:-val_num]\n","decoder_input_train = decoder_input[:-val_num]\n","decoder_target_train = decoder_target[:-val_num]\n","\n","encoder_input_test = encoder_input[-val_num:]\n","decoder_input_test = decoder_input[-val_num:]\n","decoder_target_test = decoder_target[-val_num:]"],"metadata":{"id":"kTel2uR90fRg","executionInfo":{"status":"ok","timestamp":1649217133864,"user_tz":-540,"elapsed":1,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["print('훈련 source 데이터의 크기 :',encoder_input_train.shape)\n","print('훈련 target 데이터의 크기 :',decoder_input_train.shape)\n","print('훈련 target 레이블의 크기 :',decoder_target_train.shape)\n","print('테스트 source 데이터의 크기 :',encoder_input_test.shape)\n","print('테스트 target 데이터의 크기 :',decoder_input_test.shape)\n","print('테스트 target 레이블의 크기 :',decoder_target_test.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XGLmZFeb3FgA","executionInfo":{"status":"ok","timestamp":1649217134825,"user_tz":-540,"elapsed":6,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}},"outputId":"7b24b885-8a54-4af4-ce24-0b317f6be024"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["훈련 source 데이터의 크기 : (36000, 8)\n","훈련 target 데이터의 크기 : (36000, 16)\n","훈련 target 레이블의 크기 : (36000, 16)\n","테스트 source 데이터의 크기 : (4000, 8)\n","테스트 target 데이터의 크기 : (4000, 16)\n","테스트 target 레이블의 크기 : (4000, 16)\n"]}]},{"cell_type":"markdown","source":["##본격적인 lstm Seq2Seq 모델링\n","\n","functional API로 쌓음  아래에 functional API에 대한 설명 있음\n","\n","https://www.tensorflow.org/guide/keras/functional?hl=ko"],"metadata":{"id":"KyEZdCrX6rxE"}},{"cell_type":"code","source":["embedding_dim = 64\n","hidden_units = 64"],"metadata":{"id":"eHHotIaP3vSi","executionInfo":{"status":"ok","timestamp":1649217136931,"user_tz":-540,"elapsed":9,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["# 인코더\n","encoder_inputs = Input(shape=(None,))\n","enc_emb = Embedding(src_vocab_size, embedding_dim)(encoder_inputs) # 임베딩 층\n","enc_masking = Masking(mask_value=0.0)(enc_emb) # 패딩 0은 연산에서 제외 #masgking이라는게 있는데 패딩시 발생한 0을 연산에서 아예 무시\n","encoder_lstm = LSTM(hidden_units, return_state=True,return_sequences=True) # encoder의 상태를 decoder로 보내려면 return state가 true여야한다.\n","encoder_outputs, state_h, state_c = encoder_lstm(enc_masking) # 은닉 상태와 셀 상태를 리턴\n","encoder_states = [state_h, state_c] # 인코더의 은닉 상태와 셀 상태를 저장\n","\n"],"metadata":{"id":"8BulRhv86wyQ","executionInfo":{"status":"ok","timestamp":1649217339132,"user_tz":-540,"elapsed":1713,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["encoder_outputs.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DUEiMDu6gJmo","executionInfo":{"status":"ok","timestamp":1649212020338,"user_tz":-540,"elapsed":7,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}},"outputId":"7619e6cf-0b1c-4e3b-827e-c97af21951c3"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TensorShape([None, None, 64])"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["# 디코더\n","decoder_inputs = Input(shape=(None,))\n","dec_emb_layer = Embedding(tar_vocab_size, hidden_units) # 임베딩 층\n","dec_emb = dec_emb_layer(decoder_inputs) # 패딩 0은 연산에서 제외\n","dec_masking = Masking(mask_value=0.0)(dec_emb)\n","\n","# 상태값 리턴을 위해 return_state는 True, 모든 시점에 대해서 단어를 예측하기 위해 return_sequences는 True\n","decoder_lstm = LSTM(hidden_units, return_sequences=True, return_state=True)  \n","\n","# 인코더의 은닉 상태를 초기 은닉 상태(initial_state)로 사용\n","decoder_outputs, attention_h, _ = decoder_lstm(dec_masking,\n","                                     initial_state=encoder_states)\n","\n","context_vector = Attention()([encoder_outputs,decoder_outputs])\n","\n","\n","\n","concatenate = Concatenate()([context_vector,decoder_outputs])\n","\n","# 모든 시점의 결과에 대해서 소프트맥스 함수를 사용한 출력층을 통해 단어 예측\n","decoder_dense = Dense(tar_vocab_size, activation='softmax') #vocab size만큼의 단어 분포 나오고 거기서 softmax\n","decoder_outputs=decoder_dense(concatenate)\n","\n","\n","\n","# 모델의 입력과 출력을 정의.\n","model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n","\n","model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])\n","#sparse cross entropy는 결과 값을 one-hot coding하지 않았을 때 cross entropy를 구해주는 loss function"],"metadata":{"id":"yOXL9XES9QUJ","executionInfo":{"status":"error","timestamp":1649218781868,"user_tz":-540,"elapsed":4,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}},"colab":{"base_uri":"https://localhost:8080/","height":245},"outputId":"ad483648-60bb-48ab-88bb-0bbe541c48f7"},"execution_count":2,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-fc1671a73e12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 디코더\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdecoder_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdec_emb_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtar_vocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_units\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 임베딩 층\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdec_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdec_emb_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_inputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 패딩 0은 연산에서 제외\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdec_masking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMasking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_emb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'Input' is not defined"]}]},{"cell_type":"code","source":["print(context_vector.shape,decoder_outputs.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":172},"id":"WwPmAzJprHhW","executionInfo":{"status":"error","timestamp":1649218775148,"user_tz":-540,"elapsed":15,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}},"outputId":"583e7b21-1c43-4bdc-8b5d-125544269c9a"},"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-eea19f6f834e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext_vector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdecoder_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'context_vector' is not defined"]}]},{"cell_type":"code","source":["model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tl769iR5boCw","executionInfo":{"status":"ok","timestamp":1649217358771,"user_tz":-540,"elapsed":258,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}},"outputId":"f02c1eb6-8ca1-44b4-a90f-5da52bc41774"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_2 (InputLayer)           [(None, None)]       0           []                               \n","                                                                                                  \n"," input_4 (InputLayer)           [(None, None)]       0           []                               \n","                                                                                                  \n"," embedding_1 (Embedding)        (None, None, 64)     335360      ['input_2[0][0]']                \n","                                                                                                  \n"," embedding_3 (Embedding)        (None, None, 64)     579072      ['input_4[0][0]']                \n","                                                                                                  \n"," masking_1 (Masking)            (None, None, 64)     0           ['embedding_1[0][0]']            \n","                                                                                                  \n"," masking_3 (Masking)            (None, None, 64)     0           ['embedding_3[0][0]']            \n","                                                                                                  \n"," lstm_1 (LSTM)                  [(None, None, 64),   33024       ['masking_1[0][0]']              \n","                                 (None, 64),                                                      \n","                                 (None, 64)]                                                      \n","                                                                                                  \n"," lstm_3 (LSTM)                  [(None, None, 64),   33024       ['masking_3[0][0]',              \n","                                 (None, 64),                      'lstm_1[0][1]',                 \n","                                 (None, 64)]                      'lstm_1[0][2]']                 \n","                                                                                                  \n"," dense (Dense)                  (None, None, 9048)   588120      ['lstm_3[0][0]']                 \n","                                                                                                  \n","==================================================================================================\n","Total params: 1,568,600\n","Trainable params: 1,568,600\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"]}]},{"cell_type":"code","source":["model.fit(x=[encoder_input_train, decoder_input_train], y=decoder_target_train, \\\n","          validation_data=([encoder_input_test, decoder_input_test], decoder_target_test),\n","          batch_size=128, epochs=30)"],"metadata":{"id":"vLmDMNJiAfw7","colab":{"base_uri":"https://localhost:8080/","height":744},"executionInfo":{"status":"error","timestamp":1649217397767,"user_tz":-540,"elapsed":2842,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}},"outputId":"0eb2be1e-226a-4467-b978-3e82ac6fd84f"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/30\n"]},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-318cadabf53e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model.fit(x=[encoder_input_train, decoder_input_train], y=decoder_target_train,           validation_data=([encoder_input_test, decoder_input_test], decoder_target_test),\n\u001b[0;32m----> 2\u001b[0;31m           batch_size=128, epochs=30)\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1148\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 859, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.7/dist-packages/keras/backend.py\", line 3313, in concatenate\n        return tf.concat([to_dense(x) for x in tensors], axis)\n\n    ValueError: Exception encountered when calling layer \"concatenate_1\" (type Concatenate).\n    \n    Dimension 1 in both shapes must be equal, but are 8 and 16. Shapes are [?,8] and [?,16]. for '{{node model_1/concatenate_1/concat}} = ConcatV2[N=2, T=DT_FLOAT, Tidx=DT_INT32](model_1/attention_2/mul_1, model_1/lstm_4/PartitionedCall:1, model_1/concatenate_1/concat/axis)' with input shapes: [?,8,64], [?,16,64], [] and with computed input tensors: input[2] = <2>.\n    \n    Call arguments received:\n      • inputs=['tf.Tensor(shape=(None, 8, 64), dtype=float32)', 'tf.Tensor(shape=(None, 16, 64), dtype=float32)']\n"]}]},{"cell_type":"markdown","source":["## 학습 이후 학습 된 모델로 machine translation 실행"],"metadata":{"id":"-abSS9tRBg5S"}},{"cell_type":"code","source":["# 인코더는 train에서 사용되고 이미 train된 애들을 그대로 가져옵니다\n","encoder_model = Model(encoder_inputs, encoder_states)\n","\n","# 디코더 설계 시작\n","# 이전 시점의 상태를 보관할 텐서\n","decoder_state_input_h = Input(shape=(hidden_units,))\n","decoder_state_input_c = Input(shape=(hidden_units,))\n","decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n","\n","# 훈련 때 사용했던 임베딩 층을 재사용\n","dec_emb2 = dec_emb_layer(decoder_inputs)\n","\n","# 다음 단어 예측을 위해 이전 시점의 상태를 현 시점의 초기 상태로 사용\n","decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\n","#initial state에 decoder state input h decoder state input c를 넣어줌 \n","decoder_states2 = [state_h2, state_c2]\n","\n","# 모든 시점에 대해서 단어 예측\n","decoder_outputs2 = decoder_dense(decoder_outputs2) #output을 desne에 넣어서 확률 처리\n","\n","# 수정된 디코더\n","decoder_model = Model(\n","    [decoder_inputs] + decoder_states_inputs,\n","    [decoder_outputs2] + decoder_states2)"],"metadata":{"id":"qOBVmsreBlQ3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#디코더를 컨트롤하기 위한 함수, test과정에서 사용할 함수\n","\n","def decode_sequence(input_seq):\n","  # 입력으로부터 인코더의 마지막 시점의 상태(은닉 상태, 셀 상태)를 얻음\n","  states_value = encoder_model.predict(input_seq)\n","\n","  # <SOS>에 해당하는 정수 생성\n","  target_seq = np.zeros((1,1))\n","  target_seq[0, 0] = tar_to_index['<sos>'] #시작토큰의 inex를 시작에 넣어줌\n","\n","  stop_condition = False\n","  decoded_sentence = ''\n","\n","  # stop_condition이 True가 될 때까지 루프 반복\n","  # 구현의 간소화를 위해서 이 함수는 배치 크기를 1로 가정합니다.\n","  while not stop_condition:\n","    # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n","    output_tokens, h, c = decoder_model.predict([target_seq] + states_value) \n","\n","    # 예측 결과를 단어로 변환\n","    sampled_token_index = np.argmax(output_tokens[0, -1, :])\n","    sampled_char = index_to_tar[sampled_token_index]\n","\n","    # 현재 시점의 예측 단어를 예측 문장에 추가\n","    decoded_sentence += ' '+sampled_char\n","\n","    # <eos>에 도달하거나 정해진 길이를 넘으면 중단.\n","    if (sampled_char == '<eos>' or\n","        len(decoded_sentence) > 50):\n","        stop_condition = True\n","\n","    # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n","    target_seq = np.zeros((1,1))\n","    target_seq[0, 0] = sampled_token_index\n","\n","    # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n","    states_value = [h, c]\n","\n","  return decoded_sentence"],"metadata":{"id":"ysktHo6wXCMX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 원문의 정수 시퀀스를 텍스트 시퀀스로 변환\n","def seq_to_src(input_seq):\n","  sentence = ''\n","  for encoded_word in input_seq:\n","    if(encoded_word != 0):\n","      sentence = sentence + index_to_src[encoded_word] + ' '\n","  return sentence\n","\n","# 번역문의 정수 시퀀스를 텍스트 시퀀스로 변환\n","def seq_to_tar(input_seq):\n","  sentence = ''\n","  for encoded_word in input_seq:\n","    if(encoded_word != 0 and encoded_word != tar_to_index['<sos>'] and encoded_word != tar_to_index['<eos>']):\n","      sentence = sentence + index_to_tar[encoded_word] + ' '\n","  return sentence"],"metadata":{"id":"b2CA_lYdZssM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for seq_index in [3, 50, 100, 300, 1001]:\n","  input_seq = encoder_input_train[seq_index: seq_index + 1]\n","  decoded_sentence = decode_sequence(input_seq)\n","\n","  print(\"입력문장 :\",seq_to_src(encoder_input_train[seq_index]))\n","  print(\"정답문장 :\",seq_to_tar(decoder_input_train[seq_index]))\n","  print(\"번역문장 :\",decoded_sentence[1:-5])\n","  print('BLEU :',bleu.sentence_bleu(list(map(lambda ref: ref.split(), seq_to_tar(decoder_input_train[seq_index]))),decoded_sentence[1:-5].split()))\n","  print(\"-\"*50)"],"metadata":{"id":"Gc_5xnbXaXmT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649212720455,"user_tz":-540,"elapsed":4985,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}},"outputId":"6e112b07-531c-4d27-b8f1-6233bd8c28cc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["입력문장 : it s not a weapon . \n","정답문장 : il ne s agit pas d une arme . \n","번역문장 : ce n est pas une arme . \n","BLEU : 0.7311104457090247\n","--------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n","Corpus/Sentence contains 0 counts of 2-gram overlaps.\n","BLEU scores might be undesirable; use SmoothingFunction().\n","  warnings.warn(_msg)\n"]},{"output_type":"stream","name":"stdout","text":["입력문장 : i d appreciate it . \n","정답문장 : je l apprecierais . \n","번역문장 : je l ai . \n","BLEU : 0.8408964152537145\n","--------------------------------------------------\n","입력문장 : air is invisible . \n","정답문장 : l air est invisible . \n","번역문장 : l air est fort . \n","BLEU : 0.7952707287670506\n","--------------------------------------------------\n","입력문장 : you look perfect . \n","정답문장 : tu as l air parfait . \n","번역문장 : vous avez l air parfaite . \n","BLEU : 0.7598356856515925\n","--------------------------------------------------\n","입력문장 : tom has died . \n","정답문장 : tom est decede . \n","번역문장 : tom est mort . \n","BLEU : 0.7071067811865476\n","--------------------------------------------------\n"]}]},{"cell_type":"code","source":["for seq_index in [3, 50, 100, 300, 1001]:\n","  input_seq = encoder_input_test[seq_index: seq_index + 1]\n","  decoded_sentence = decode_sequence(input_seq)\n","\n","  print(\"입력문장 :\",seq_to_src(encoder_input_test[seq_index]))\n","  print(\"정답문장 :\",seq_to_tar(decoder_input_test[seq_index]))\n","  print(\"번역문장 :\",decoded_sentence[1:-5])\n","  print('BLEU :',bleu.sentence_bleu(list(map(lambda ref: ref.split(), seq_to_tar(decoder_input_test[seq_index]))),decoded_sentence[1:-5].split()))\n","\n","  print(\"-\"*50)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DOGEwKWFdMwt","executionInfo":{"status":"ok","timestamp":1649212723774,"user_tz":-540,"elapsed":3073,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}},"outputId":"9380073e-d6b6-4833-c966-badd8790c957"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["입력문장 : that s one of them . \n","정답문장 : il s agit de l une des leurs . \n","번역문장 : il y a la . \n","BLEU : 0.7952707287670506\n","--------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n","Corpus/Sentence contains 0 counts of 2-gram overlaps.\n","BLEU scores might be undesirable; use SmoothingFunction().\n","  warnings.warn(_msg)\n"]},{"output_type":"stream","name":"stdout","text":["입력문장 : what do you need ? \n","정답문장 : de quoi avez vous besoin ? \n","번역문장 : qu as tu besoin ? \n","BLEU : 0.668740304976422\n","--------------------------------------------------\n","입력문장 : tom knows who i am . \n","정답문장 : tom sait qui je suis . \n","번역문장 : tom n est ce que je suis . \n","BLEU : 0.5946035575013605\n","--------------------------------------------------\n","입력문장 : tom became very ill . \n","정답문장 : tom devint tres malade . \n","번역문장 : tom est tres bien . \n","BLEU : 0.668740304976422\n","--------------------------------------------------\n","입력문장 : tom is back in town . \n","정답문장 : tom est de retour en ville . \n","번역문장 : tom a l heure de l interieur . \n","BLEU : 0.7071067811865476\n","--------------------------------------------------\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"NovNpHFBr_Bx"},"execution_count":null,"outputs":[]}]}