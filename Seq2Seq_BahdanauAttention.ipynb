{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Seq2Seq_BahdanauAttention.ipynb","provenance":[{"file_id":"14RlYWyvQWEzG7pOHbOLBPM_wCCMv9oUG","timestamp":1649169183961}],"collapsed_sections":[],"mount_file_id":"14RlYWyvQWEzG7pOHbOLBPM_wCCMv9oUG","authorship_tag":"ABX9TyPnWoZn/qLWDXe1W8e8ZXUu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"QlNRln8ij7mC","executionInfo":{"status":"ok","timestamp":1649219049196,"user_tz":-540,"elapsed":5970,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}}},"outputs":[],"source":["import os\n","import re\n","\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","import unicodedata\n","import urllib3\n","from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Masking, Concatenate\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","\n","import nltk.translate.bleu_score as bleu\n","from collections import Counter\n","from nltk import ngrams"]},{"cell_type":"markdown","source":["##학습을 위한 병렬 corpus 가져오기\n","source와 target을 각각 병렬적으로 매치한 데이터셋"],"metadata":{"id":"pIh9-gtZryQQ"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"22XaDPjzkKNE","executionInfo":{"status":"ok","timestamp":1649219043231,"user_tz":-540,"elapsed":18604,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}},"outputId":"f3589f59-4313-4a2d-db3f-09fb67da1974"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!unzip -uq \"/content/drive/MyDrive/fra-eng.zip\" -d \"/content\" \n","\n","#영어와 프랑스어를 매치한 데이터 셋\n","#영어가 source 프랑스어가 target"],"metadata":{"id":"ctZi2Qd0kiz0","executionInfo":{"status":"ok","timestamp":1649219049851,"user_tz":-540,"elapsed":669,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["def to_ascii(s):\n","  # 프랑스어 악센트(accent) 삭제\n","  # 예시 : 'déjà diné' -> deja dine\n","  return ''.join(c for c in unicodedata.normalize('NFD', s)\n","                   if unicodedata.category(c) != 'Mn')\n","\n","def preprocess_sentence(sent):\n","  # 악센트 제거 함수 호출\n","  sent = to_ascii(sent.lower())\n","\n","  # 단어와 구두점 사이에 공백 추가해서 구두점을 구분\n","  # ex) \"I am a student.\" => \"I am a student .\"\n","  sent = re.sub(r\"([?.!,¿])\", r\" \\1\", sent)\n","\n","  # (a-z, A-Z, \".\", \"?\", \"!\", \",\")  영어랑 . ? ! , 제외하고 모두 지움\n","  sent = re.sub(r\"[^a-zA-Z!.?]+\", r\" \", sent)\n","\n","  # 다수 개의 공백을 하나의 공백으로 치환\n","  sent = re.sub(r\"\\s+\", \" \", sent)\n","  return sent"],"metadata":{"id":"ifDy9PUgoUST","executionInfo":{"status":"ok","timestamp":1649219050233,"user_tz":-540,"elapsed":385,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["num_samples =40000 #원래는 19만개의 문장이 있는데 3만개정도만 사용\n","\n","def load_preprocessed_data():\n","  encoder_input, decoder_input, decoder_target = [], [], [] \n","  #input을 3개 만듦 encoder는 output이 없으므로 encoder target은 만들 필요 x\n","  with open(\"fra.txt\", \"r\") as lines:\n","    for i, line in enumerate(lines): #line 하나 안에 tab을 기준으로 source와 target을 구분하고 있음\n","      # source 데이터와 target 데이터를 tab을 기준으로 분리\n","      src_line, tar_line, _ = line.strip().split('\\t')\n","\n","      # source 데이터 전처리\n","      src_line = [w for w in preprocess_sentence(src_line).split()]\n","\n","      # target 데이터 전처리\n","      tar_line = preprocess_sentence(tar_line)\n","      tar_line_in = [w for w in (\"<sos> \" + tar_line).split()] #line을 받아와서 sos 토큰을 넣어준다\n","      tar_line_out = [w for w in (tar_line + \" <eos>\").split()] #line을 받아와서 eos 토큰을 넣어준다.\n","\n","      encoder_input.append(src_line)\n","      decoder_input.append(tar_line_in)\n","      decoder_target.append(tar_line_out)\n","\n","      if i == num_samples - 1:\n","        break\n","  lines.close()\n","  return encoder_input, decoder_input, decoder_target\n","\n","sents_en_in , sents_fra_in, sents_fra_out = load_preprocessed_data()"],"metadata":{"id":"q0ltJWJ3mAwE","executionInfo":{"status":"ok","timestamp":1649219052191,"user_tz":-540,"elapsed":1963,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["print('인코더의 입력 :',sents_en_in[15])\n","print('디코더의 입력 :',sents_fra_in[15])\n","print('디코더의 레이블 :',sents_fra_out[15])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YV53HrgUuJee","executionInfo":{"status":"ok","timestamp":1649219052192,"user_tz":-540,"elapsed":9,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}},"outputId":"145a89d0-2882-4369-e92c-a974cef07910"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["인코더의 입력 : ['run', '.']\n","디코더의 입력 : ['<sos>', 'prenez', 'vos', 'jambes', 'a', 'vos', 'cous', '!']\n","디코더의 레이블 : ['prenez', 'vos', 'jambes', 'a', 'vos', 'cous', '!', '<eos>']\n"]}]},{"cell_type":"markdown","source":["##Seq2Seq에 넣기 위해서 tokenize\n","\n","영어와 프랑스어 각각에 대해서 tokenize함\n"],"metadata":{"id":"LBBnRubLwq7M"}},{"cell_type":"code","source":["#tokenizer 안에 filters는 문장 내에서 \"\"안에 것을 filtering함 우리는 이미 전처리 다해놔서 거를 것 없음 lower도 이미 해놓음\n","\n","tokenizer_en = Tokenizer(filters=\"\", lower=False)\n","tokenizer_en.fit_on_texts(sents_en_in)\n","\n","tokenizer_fra = Tokenizer(filters=\"\", lower=False)\n","tokenizer_fra.fit_on_texts(sents_fra_in)\n","tokenizer_fra.fit_on_texts(sents_fra_out)\n","\n","\n","encoder_input = tokenizer_en.texts_to_sequences(sents_en_in)\n","encoder_input = pad_sequences(encoder_input, padding=\"post\",maxlen=16)\n","#padding에는 2가지 pre, post 존재 post는 0들을 뒤에 채우는 것\n","\n","decoder_input = tokenizer_fra.texts_to_sequences(sents_fra_in)\n","decoder_input = pad_sequences(decoder_input, padding=\"post\")\n","\n","decoder_target = tokenizer_fra.texts_to_sequences(sents_fra_out)\n","decoder_target = pad_sequences(decoder_target, padding=\"post\")"],"metadata":{"id":"XP8PCJ5FvuPT","executionInfo":{"status":"ok","timestamp":1649224628184,"user_tz":-540,"elapsed":1470,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}}},"execution_count":80,"outputs":[]},{"cell_type":"code","source":["#영어는 문장 최대 길이가 8인 반면 프랑스어는 최대 길이가 16\n","\n","print('인코더의 입력의 크기(shape) :',encoder_input.shape)\n","print('디코더의 입력의 크기(shape) :',decoder_input.shape)\n","print('디코더의 레이블의 크기(shape) :',decoder_target.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Gj0X4fGnx3cP","executionInfo":{"status":"ok","timestamp":1649224630337,"user_tz":-540,"elapsed":253,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}},"outputId":"ae77e61b-bd60-41db-e71d-64d540aa65af"},"execution_count":81,"outputs":[{"output_type":"stream","name":"stdout","text":["인코더의 입력의 크기(shape) : (40000, 16)\n","디코더의 입력의 크기(shape) : (40000, 16)\n","디코더의 레이블의 크기(shape) : (40000, 16)\n"]}]},{"cell_type":"code","source":["#word_index는 각 단어에 대한 index를 매칭해서 dictionary로 반환\n","src_vocab_size = len(tokenizer_en.word_index) + 1\n","tar_vocab_size = len(tokenizer_fra.word_index) + 1\n","print(\"영어 단어 집합의 크기 : {:d}, 프랑스어 단어 집합의 크기 : {:d}\".format(src_vocab_size, tar_vocab_size))"],"metadata":{"id":"3qUWf1Pcx9y2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649224631789,"user_tz":-540,"elapsed":3,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}},"outputId":"91ea5b5a-a603-47f0-f70e-17c0f8855762"},"execution_count":82,"outputs":[{"output_type":"stream","name":"stdout","text":["영어 단어 집합의 크기 : 5240, 프랑스어 단어 집합의 크기 : 9048\n"]}]},{"cell_type":"code","source":["#word_index는 단어 - 인덱스 순의 dictionary\n","#index_word는 그 반대 아래의 딕셔너리는 이후 예측값과 실제값 예측에 사용\n","\n","src_to_index = tokenizer_en.word_index\n","index_to_src = tokenizer_en.index_word\n","tar_to_index = tokenizer_fra.word_index\n","index_to_tar = tokenizer_fra.index_word"],"metadata":{"id":"QQI6x7IOyJ9t","executionInfo":{"status":"ok","timestamp":1649224632794,"user_tz":-540,"elapsed":3,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}}},"execution_count":83,"outputs":[]},{"cell_type":"code","source":["#33000개의 문장을 무작위로 shuffle\n","indices = np.arange(num_samples)\n","np.random.shuffle(indices)\n","encoder_input = encoder_input[indices]\n","decoder_input = decoder_input[indices]\n","decoder_target = decoder_target[indices]"],"metadata":{"id":"wfbz9UKPyy0Z","executionInfo":{"status":"ok","timestamp":1649224633402,"user_tz":-540,"elapsed":5,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}}},"execution_count":84,"outputs":[]},{"cell_type":"code","source":["val_num=int(0.1*num_samples) #10% 만큼 test에 사용 split 해준다\n","encoder_input_train = encoder_input[:-val_num]\n","decoder_input_train = decoder_input[:-val_num]\n","decoder_target_train = decoder_target[:-val_num]\n","\n","encoder_input_test = encoder_input[-val_num:]\n","decoder_input_test = decoder_input[-val_num:]\n","decoder_target_test = decoder_target[-val_num:]"],"metadata":{"id":"kTel2uR90fRg","executionInfo":{"status":"ok","timestamp":1649224634525,"user_tz":-540,"elapsed":4,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}}},"execution_count":85,"outputs":[]},{"cell_type":"code","source":["print('훈련 source 데이터의 크기 :',encoder_input_train.shape)\n","print('훈련 target 데이터의 크기 :',decoder_input_train.shape)\n","print('훈련 target 레이블의 크기 :',decoder_target_train.shape)\n","print('테스트 source 데이터의 크기 :',encoder_input_test.shape)\n","print('테스트 target 데이터의 크기 :',decoder_input_test.shape)\n","print('테스트 target 레이블의 크기 :',decoder_target_test.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XGLmZFeb3FgA","executionInfo":{"status":"ok","timestamp":1649224635408,"user_tz":-540,"elapsed":4,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}},"outputId":"94186ca3-ad08-455b-d560-9fa9e98f86f6"},"execution_count":86,"outputs":[{"output_type":"stream","name":"stdout","text":["훈련 source 데이터의 크기 : (36000, 16)\n","훈련 target 데이터의 크기 : (36000, 16)\n","훈련 target 레이블의 크기 : (36000, 16)\n","테스트 source 데이터의 크기 : (4000, 16)\n","테스트 target 데이터의 크기 : (4000, 16)\n","테스트 target 레이블의 크기 : (4000, 16)\n"]}]},{"cell_type":"markdown","source":["##본격적인 lstm Seq2Seq 모델링\n","\n","functional API로 쌓음  아래에 functional API에 대한 설명 있음\n","\n","https://www.tensorflow.org/guide/keras/functional?hl=ko"],"metadata":{"id":"KyEZdCrX6rxE"}},{"cell_type":"code","source":["embedding_dim = 64\n","hidden_units = 64"],"metadata":{"id":"eHHotIaP3vSi","executionInfo":{"status":"ok","timestamp":1649224637027,"user_tz":-540,"elapsed":314,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}}},"execution_count":87,"outputs":[]},{"cell_type":"code","source":["class BahdanauAttention(tf.keras.Model):\n","  def __init__(self, units):\n","    super(BahdanauAttention, self).__init__()\n","    self.W1 = Dense(units)\n","    self.W2 = Dense(units)\n","    self.V = Dense(units)\n","\n","  def call(self, values, query): # 단, key와 value는 같음\n","    # query shape == (batch_size, hidden size)\n","    # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n","    # score 계산을 위해 뒤에서 할 덧셈을 위해서 차원을 변경해줍니다.\n","    hidden_with_time_axis = tf.expand_dims(query, 1)\n","\n","    # score shape == (batch_size, max_length, 1)\n","    # we get 1 at the last axis because we are applying score to self.V\n","    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n","    score = self.V(tf.nn.tanh(\n","        self.W1(values) + self.W2(hidden_with_time_axis)))\n","\n","    # attention_weights shape == (batch_size, max_length, 1)\n","    attention_weights = tf.nn.softmax(score, axis=1)\n","\n","    # context_vector shape after sum == (batch_size, hidden_size)\n","    context_vector = attention_weights * values\n","    context_vector = tf.reduce_sum(context_vector, axis=1)\n","\n","    return context_vector,attention_weights"],"metadata":{"id":"-8qzjAtcQGqT","executionInfo":{"status":"ok","timestamp":1649224637989,"user_tz":-540,"elapsed":6,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}}},"execution_count":88,"outputs":[]},{"cell_type":"code","source":["# 인코더\n","encoder_inputs = Input(shape=(None,))\n","enc_emb = Embedding(src_vocab_size, embedding_dim)(encoder_inputs) # 임베딩 층\n","enc_masking = Masking(mask_value=0.0)(enc_emb) # 패딩 0은 연산에서 제외 #masgking이라는게 있는데 패딩시 발생한 0을 연산에서 아예 무시\n","encoder_lstm = LSTM(hidden_units, return_state=True,return_sequences=True) # encoder의 상태를 decoder로 보내려면 return state가 true여야한다.\n","encoder_outputs, state_h, state_c = encoder_lstm(enc_masking) # 은닉 상태와 셀 상태를 리턴\n","encoder_states = [state_h, state_c] # 인코더의 은닉 상태와 셀 상태를 저장\n","\n"],"metadata":{"id":"8BulRhv86wyQ","executionInfo":{"status":"ok","timestamp":1649224641045,"user_tz":-540,"elapsed":1359,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}}},"execution_count":89,"outputs":[]},{"cell_type":"code","source":["encoder_outputs.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DUEiMDu6gJmo","executionInfo":{"status":"ok","timestamp":1649224642353,"user_tz":-540,"elapsed":238,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}},"outputId":"07569e60-66bd-4b12-ff4b-04441eefa154"},"execution_count":90,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TensorShape([None, None, 64])"]},"metadata":{},"execution_count":90}]},{"cell_type":"code","source":["# 디코더\n","decoder_inputs = Input(shape=(None,))\n","dec_emb_layer = Embedding(tar_vocab_size, embedding_dim) # 임베딩 층\n","dec_emb = dec_emb_layer(decoder_inputs) # 패딩 0은 연산에서 제외\n","dec_masking = Masking(mask_value=0.0)(dec_emb)\n","\n","# 상태값 리턴을 위해 return_state는 True, 모든 시점에 대해서 단어를 예측하기 위해 return_sequences는 True\n","decoder_lstm = LSTM(hidden_units, return_sequences=True, return_state=True)  \n","\n","# 인코더의 은닉 상태를 초기 은닉 상태(initial_state)로 사용\n","decoder_outputs, attention_h, _ = decoder_lstm(dec_masking,\n","                                     initial_state=encoder_states)\n","\n","attention = BahdanauAttention(hidden_units) # 가중치 크기 정의\n","context_vector, attention_weights = attention(encoder_outputs,decoder_outputs)\n","\n","concatenate = Concatenate(axis=-1)([context_vector,decoder_outputs])\n","\n","# 모든 시점의 결과에 대해서 소프트맥스 함수를 사용한 출력층을 통해 단어 예측\n","decoder_dense = Dense(tar_vocab_size, activation='softmax') #vocab size만큼의 단어 분포 나오고 거기서 softmax\n","attention_outputs=decoder_dense(concatenate)\n","\n","\n","\n","# 모델의 입력과 출력을 정의.\n","model = Model([encoder_inputs, decoder_inputs], attention_outputs)\n","\n","model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])\n","#sparse cross entropy는 결과 값을 one-hot coding하지 않았을 때 cross entropy를 구해주는 loss function"],"metadata":{"id":"yOXL9XES9QUJ","executionInfo":{"status":"ok","timestamp":1649224644908,"user_tz":-540,"elapsed":1583,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}}},"execution_count":91,"outputs":[]},{"cell_type":"code","source":["print(context_vector.shape, decoder_outputs.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WwPmAzJprHhW","executionInfo":{"status":"ok","timestamp":1649224644909,"user_tz":-540,"elapsed":11,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}},"outputId":"dcdf067d-72a2-4c49-d781-1eb06764e56b"},"execution_count":92,"outputs":[{"output_type":"stream","name":"stdout","text":["(None, None, 64) (None, None, 64)\n"]}]},{"cell_type":"code","source":["model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tl769iR5boCw","executionInfo":{"status":"ok","timestamp":1649224647372,"user_tz":-540,"elapsed":244,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}},"outputId":"645ac61f-e955-4608-b8b1-7273c6dc0070"},"execution_count":93,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_8\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_26 (InputLayer)          [(None, None)]       0           []                               \n","                                                                                                  \n"," embedding_25 (Embedding)       (None, None, 64)     335360      ['input_26[0][0]']               \n","                                                                                                  \n"," input_27 (InputLayer)          [(None, None)]       0           []                               \n","                                                                                                  \n"," masking_25 (Masking)           (None, None, 64)     0           ['embedding_25[0][0]']           \n","                                                                                                  \n"," embedding_26 (Embedding)       (None, None, 64)     579072      ['input_27[0][0]']               \n","                                                                                                  \n"," lstm_25 (LSTM)                 [(None, None, 64),   33024       ['masking_25[0][0]']             \n","                                 (None, 64),                                                      \n","                                 (None, 64)]                                                      \n","                                                                                                  \n"," masking_26 (Masking)           (None, None, 64)     0           ['embedding_26[0][0]']           \n","                                                                                                  \n"," lstm_26 (LSTM)                 [(None, None, 64),   33024       ['masking_26[0][0]',             \n","                                 (None, 64),                      'lstm_25[0][1]',                \n","                                 (None, 64)]                      'lstm_25[0][2]']                \n","                                                                                                  \n"," bahdanau_attention_16 (Bahdana  ((None, None, 64),  12480       ['lstm_25[0][0]',                \n"," uAttention)                     (None, None, None,               'lstm_26[0][0]']                \n","                                 64))                                                             \n","                                                                                                  \n"," concatenate_12 (Concatenate)   (None, None, 128)    0           ['bahdanau_attention_16[0][0]',  \n","                                                                  'lstm_26[0][0]']                \n","                                                                                                  \n"," dense_59 (Dense)               (None, None, 9048)   1167192     ['concatenate_12[0][0]']         \n","                                                                                                  \n","==================================================================================================\n","Total params: 2,160,152\n","Trainable params: 2,160,152\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"]}]},{"cell_type":"code","source":["model.fit(x=[encoder_input_train, decoder_input_train], y=decoder_target_train, \\\n","          validation_data=([encoder_input_test, decoder_input_test], decoder_target_test),\n","          batch_size=128, epochs=30)"],"metadata":{"id":"vLmDMNJiAfw7","colab":{"base_uri":"https://localhost:8080/"},"outputId":"bac7308f-f26b-4d30-d923-b042334c4891"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/30\n","282/282 [==============================] - 51s 130ms/step - loss: 2.8130 - acc: 0.6347 - val_loss: 1.8885 - val_acc: 0.6714\n","Epoch 2/30\n","282/282 [==============================] - 32s 114ms/step - loss: 1.6943 - acc: 0.7292 - val_loss: 1.6327 - val_acc: 0.7352\n","Epoch 3/30\n"," 18/282 [>.............................] - ETA: 28s - loss: 1.5968 - acc: 0.7400"]}]},{"cell_type":"markdown","source":["## 학습 이후 학습 된 모델로 machine translation 실행"],"metadata":{"id":"-abSS9tRBg5S"}},{"cell_type":"code","source":["# 인코더는 train에서 사용되고 이미 train된 애들을 그대로 가져옵니다\n","encoder_model = Model(encoder_inputs, encoder_states)\n","\n","# 디코더 설계 시작\n","# 이전 시점의 상태를 보관할 텐서\n","decoder_state_input_h = Input(shape=(hidden_units,))\n","decoder_state_input_c = Input(shape=(hidden_units,))\n","decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n","\n","# 훈련 때 사용했던 임베딩 층을 재사용\n","dec_emb2 = dec_emb_layer(decoder_inputs)\n","\n","# 다음 단어 예측을 위해 이전 시점의 상태를 현 시점의 초기 상태로 사용\n","decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\n","#initial state에 decoder state input h decoder state input c를 넣어줌 \n","decoder_states2 = [state_h2, state_c2]\n","\n","# 모든 시점에 대해서 단어 예측\n","decoder_outputs2 = decoder_dense(decoder_outputs2) #output을 desne에 넣어서 확률 처리\n","\n","# 수정된 디코더\n","decoder_model = Model(\n","    [decoder_inputs] + decoder_states_inputs,\n","    [decoder_outputs2] + decoder_states2)"],"metadata":{"id":"qOBVmsreBlQ3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#디코더를 컨트롤하기 위한 함수, test과정에서 사용할 함수\n","\n","def decode_sequence(input_seq):\n","  # 입력으로부터 인코더의 마지막 시점의 상태(은닉 상태, 셀 상태)를 얻음\n","  states_value = encoder_model.predict(input_seq)\n","\n","  # <SOS>에 해당하는 정수 생성\n","  target_seq = np.zeros((1,1))\n","  target_seq[0, 0] = tar_to_index['<sos>'] #시작토큰의 inex를 시작에 넣어줌\n","\n","  stop_condition = False\n","  decoded_sentence = ''\n","\n","  # stop_condition이 True가 될 때까지 루프 반복\n","  # 구현의 간소화를 위해서 이 함수는 배치 크기를 1로 가정합니다.\n","  while not stop_condition:\n","    # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n","    output_tokens, h, c = decoder_model.predict([target_seq] + states_value) \n","\n","    # 예측 결과를 단어로 변환\n","    sampled_token_index = np.argmax(output_tokens[0, -1, :])\n","    sampled_char = index_to_tar[sampled_token_index]\n","\n","    # 현재 시점의 예측 단어를 예측 문장에 추가\n","    decoded_sentence += ' '+sampled_char\n","\n","    # <eos>에 도달하거나 정해진 길이를 넘으면 중단.\n","    if (sampled_char == '<eos>' or\n","        len(decoded_sentence) > 50):\n","        stop_condition = True\n","\n","    # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n","    target_seq = np.zeros((1,1))\n","    target_seq[0, 0] = sampled_token_index\n","\n","    # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n","    states_value = [h, c]\n","\n","  return decoded_sentence"],"metadata":{"id":"ysktHo6wXCMX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 원문의 정수 시퀀스를 텍스트 시퀀스로 변환\n","def seq_to_src(input_seq):\n","  sentence = ''\n","  for encoded_word in input_seq:\n","    if(encoded_word != 0):\n","      sentence = sentence + index_to_src[encoded_word] + ' '\n","  return sentence\n","\n","# 번역문의 정수 시퀀스를 텍스트 시퀀스로 변환\n","def seq_to_tar(input_seq):\n","  sentence = ''\n","  for encoded_word in input_seq:\n","    if(encoded_word != 0 and encoded_word != tar_to_index['<sos>'] and encoded_word != tar_to_index['<eos>']):\n","      sentence = sentence + index_to_tar[encoded_word] + ' '\n","  return sentence"],"metadata":{"id":"b2CA_lYdZssM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for seq_index in [3, 50, 100, 300, 1001]:\n","  input_seq = encoder_input_train[seq_index: seq_index + 1]\n","  decoded_sentence = decode_sequence(input_seq)\n","\n","  print(\"입력문장 :\",seq_to_src(encoder_input_train[seq_index]))\n","  print(\"정답문장 :\",seq_to_tar(decoder_input_train[seq_index]))\n","  print(\"번역문장 :\",decoded_sentence[1:-5])\n","  print('BLEU :',bleu.sentence_bleu(list(map(lambda ref: ref.split(), seq_to_tar(decoder_input_train[seq_index]))),decoded_sentence[1:-5].split()))\n","  print(\"-\"*50)"],"metadata":{"id":"Gc_5xnbXaXmT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649212720455,"user_tz":-540,"elapsed":4985,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}},"outputId":"6e112b07-531c-4d27-b8f1-6233bd8c28cc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["입력문장 : it s not a weapon . \n","정답문장 : il ne s agit pas d une arme . \n","번역문장 : ce n est pas une arme . \n","BLEU : 0.7311104457090247\n","--------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n","Corpus/Sentence contains 0 counts of 2-gram overlaps.\n","BLEU scores might be undesirable; use SmoothingFunction().\n","  warnings.warn(_msg)\n"]},{"output_type":"stream","name":"stdout","text":["입력문장 : i d appreciate it . \n","정답문장 : je l apprecierais . \n","번역문장 : je l ai . \n","BLEU : 0.8408964152537145\n","--------------------------------------------------\n","입력문장 : air is invisible . \n","정답문장 : l air est invisible . \n","번역문장 : l air est fort . \n","BLEU : 0.7952707287670506\n","--------------------------------------------------\n","입력문장 : you look perfect . \n","정답문장 : tu as l air parfait . \n","번역문장 : vous avez l air parfaite . \n","BLEU : 0.7598356856515925\n","--------------------------------------------------\n","입력문장 : tom has died . \n","정답문장 : tom est decede . \n","번역문장 : tom est mort . \n","BLEU : 0.7071067811865476\n","--------------------------------------------------\n"]}]},{"cell_type":"code","source":["for seq_index in [3, 50, 100, 300, 1001]:\n","  input_seq = encoder_input_test[seq_index: seq_index + 1]\n","  decoded_sentence = decode_sequence(input_seq)\n","\n","  print(\"입력문장 :\",seq_to_src(encoder_input_test[seq_index]))\n","  print(\"정답문장 :\",seq_to_tar(decoder_input_test[seq_index]))\n","  print(\"번역문장 :\",decoded_sentence[1:-5])\n","  print('BLEU :',bleu.sentence_bleu(list(map(lambda ref: ref.split(), seq_to_tar(decoder_input_test[seq_index]))),decoded_sentence[1:-5].split()))\n","\n","  print(\"-\"*50)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DOGEwKWFdMwt","executionInfo":{"status":"ok","timestamp":1649212723774,"user_tz":-540,"elapsed":3073,"user":{"displayName":"Sangwook Baek","userId":"14769644796773614633"}},"outputId":"9380073e-d6b6-4833-c966-badd8790c957"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["입력문장 : that s one of them . \n","정답문장 : il s agit de l une des leurs . \n","번역문장 : il y a la . \n","BLEU : 0.7952707287670506\n","--------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n","Corpus/Sentence contains 0 counts of 2-gram overlaps.\n","BLEU scores might be undesirable; use SmoothingFunction().\n","  warnings.warn(_msg)\n"]},{"output_type":"stream","name":"stdout","text":["입력문장 : what do you need ? \n","정답문장 : de quoi avez vous besoin ? \n","번역문장 : qu as tu besoin ? \n","BLEU : 0.668740304976422\n","--------------------------------------------------\n","입력문장 : tom knows who i am . \n","정답문장 : tom sait qui je suis . \n","번역문장 : tom n est ce que je suis . \n","BLEU : 0.5946035575013605\n","--------------------------------------------------\n","입력문장 : tom became very ill . \n","정답문장 : tom devint tres malade . \n","번역문장 : tom est tres bien . \n","BLEU : 0.668740304976422\n","--------------------------------------------------\n","입력문장 : tom is back in town . \n","정답문장 : tom est de retour en ville . \n","번역문장 : tom a l heure de l interieur . \n","BLEU : 0.7071067811865476\n","--------------------------------------------------\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"NovNpHFBr_Bx"},"execution_count":null,"outputs":[]}]}